{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7ea8fba-ed8c-4a5e-8b6b-6428ff119b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8379c79-907e-4e30-9941-691e4c687178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "def export_labels_to_model(model_name: str, model) -> None:\n",
    "    \"\"\"\n",
    "    Reads from a model configuration to export the labels of the class target to a file in the model's assets folder.\n",
    "    \n",
    "    Args:\n",
    "      model_name (str): The name of the model. This is used to create a directory for the model.\n",
    "      model: The model to export.\n",
    "    \"\"\"\n",
    "    labels = model.config.label2id\n",
    "    labels = sorted(labels, key=labels.get)\n",
    "\n",
    "    model_assets_path = f'models/{model_name}/saved_model/1/assets'\n",
    "\n",
    "    with open(f'{model_assets_path}/labels.txt', 'w') as f:\n",
    "        f.write('\\n'.join(labels))\n",
    "\n",
    "def save_model_from_hub(model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    We load the model and tokenizer from the HuggingFace hub, save them to the `models` directory, and then export\n",
    "    the labels of the model to the directory that contains all the assets.\n",
    "    \n",
    "    Args:\n",
    "      model_name (str): The name of the model you want to save.\n",
    "    \"\"\"\n",
    "\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(f'models/{model_name}', from_tf=True, save_format='tf', saved_model=True)\n",
    "    tokenizer.save_pretrained(f'models/{model_name}_tokenizer', from_tf=True, save_format='tf')\n",
    "    export_labels_to_model(model_name, model)\n",
    "\n",
    "    print(f\"Model {model_name} saved.\")\n",
    "\n",
    "def copy_tokenizer_vocab_to_model(model_name):\n",
    "    \"\"\"\n",
    "    We copy the tokenizer's vocabulary to the model's directory, so that we can use the model for\n",
    "    predictions.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model you want to use.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer_vocab_path = f'models/{model_name}_tokenizer/vocab.txt'\n",
    "    model_assets_path = f'models/{model_name}/saved_model/1/assets'\n",
    "\n",
    "    shutil.copyfile(tokenizer_vocab_path, f'{model_assets_path}/vocab.txt')\n",
    "    \n",
    "\n",
    "def prepare_model_from_hub(model_name: str, model_dir:str) -> None:\n",
    "    \"\"\"\n",
    "    If the model directory doesn't exist, download the model from the HuggingFace Hub, and copy the tokenizer\n",
    "    vocab to the model directory so that the format can be digested by Spark NLP.\n",
    "    \n",
    "    Args:\n",
    "      model_name (str): The name of the model you want to use.\n",
    "      model_dir (str): The directory where the model will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = f'{model_dir}/{model_name}'\n",
    "\n",
    "    if not Path(model_path).is_dir():\n",
    "        save_model_from_hub(model_name)\n",
    "        copy_tokenizer_vocab_to_model(model_name)\n",
    "\n",
    "def get_label_metadata(dataset):\n",
    "  \"\"\"\n",
    "  It takes a dataset and returns a list of labels, a dictionary mapping label ids to labels, and a\n",
    "  dictionary mapping labels to label ids\n",
    "  \n",
    "  Args:\n",
    "    dataset: the dataset object\n",
    "  \"\"\"\n",
    "  labels = list(set(dataset['train']['label']).union(set(dataset['test']['label'])))\n",
    "  id2label = dict(enumerate(labels))\n",
    "  label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "  return labels, id2label, label2id\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  \"\"\"\n",
    "  It takes in the predictions and labels from the model, and returns a dictionary of metrics.\n",
    "  Logits are converted into probabilities following a sigmoid function; then, the predictions are\n",
    "  converted into binary values by comparing the probabilities to a threshold.\n",
    "  \n",
    "  Args:\n",
    "    eval_pred: a tuple of (predictions, labels)\n",
    "  \n",
    "  Returns:\n",
    "    A dictionary with the accuracy, f1_micro and f1_macro\n",
    "  \"\"\"\n",
    "  #sigmoid_threshold = 0.3\n",
    "  #print(eval_pred)  \n",
    "  predictions, labels = eval_pred\n",
    "  #print( predictions, labels)  \n",
    "  predicted_labels_index = np.argmax(predictions, axis=-1)\n",
    "  num_classes = predictions.shape[1]  # Assuming predictions is a 2D array where axis=1 is the number of classes\n",
    "  predicted_labels = np.eye(num_classes)[predicted_labels_index]  \n",
    "  print(predicted_labels_index,predicted_labels,labels)  \n",
    "   # Compute accuracy\n",
    "  accuracy = (predicted_labels == labels).mean()\n",
    "   # Compute F1 scores\n",
    "  f1_micro = sklearn.metrics.f1_score(labels, predicted_labels, average=\"micro\")\n",
    "  f1_macro = sklearn.metrics.f1_score(labels, predicted_labels, average=\"macro\")\n",
    "    \n",
    "  return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"eval_f1\": f1_micro\n",
    "    }\n",
    "\n",
    "def accuracy_thresh(y_pred, y_true, thresh): \n",
    "    \"\"\"\n",
    "    It takes in a predicted probability and a true label, and returns the accuracy of the prediction\n",
    "    \n",
    "    Args:\n",
    "      y_pred: the predicted values\n",
    "      y_true: the ground truth labels\n",
    "      thresh: the threshold for the prediction to be considered a positive prediction.\n",
    "    \n",
    "    Returns:\n",
    "      The mean of the accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    y_pred = torch.from_numpy(y_pred).sigmoid()\n",
    "    #print(y_pred)\n",
    "    y_true = torch.from_numpy(y_true)\n",
    "    #print(y_true)\n",
    "    return ((y_pred>thresh)==y_true.bool()).float().mean().item()\n",
    "\n",
    "\n",
    "def prepare_splits_for_training(dataset, subset_data):\n",
    "  \"\"\"Splits and shuffles the dataset into train and test splits.\n",
    "\n",
    "  Args:\n",
    "      dataset (DatasetDict): The dataset to split. \n",
    "      subset_data (bool, optional): Flag to use a subset of the data.\n",
    "\n",
    "  Returns:\n",
    "      Tuple[Dataset]: One dataset object per train, test split.\n",
    "  \"\"\"\n",
    "  fraction = 0.05 if subset_data else 1\n",
    "  splits = [dataset[\"train\"], dataset[\"test\"]]\n",
    "\n",
    "  return [\n",
    "    split.shuffle(seed=42).select(range(int(len(split) * fraction)))\n",
    "    for split in splits\n",
    "  ]\n",
    "\n",
    "def convert_to_tf_dataset(dataset, data_collator, shuffle_flag, batch_size):\n",
    "  \"\"\"\n",
    "  We convert the dataset to a tf.data.Dataset object, which is a TensorFlow object that can be used\n",
    "  to train a model\n",
    "  \n",
    "  Args:\n",
    "    dataset: The dataset to convert to a tf.data.Dataset.\n",
    "    data_collator: This is a function that takes in a list of tensors and returns a single tensor.\n",
    "    shuffle_flag: Whether to shuffle the dataset or not.\n",
    "    batch_size: The number of samples per batch.\n",
    "  \n",
    "  Returns:\n",
    "    A tf.data.Dataset object\n",
    "  \"\"\"\n",
    "  return (\n",
    "      dataset.to_tf_dataset(\n",
    "          columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "          label_cols=[\"labels\"],\n",
    "          shuffle=shuffle_flag,\n",
    "          collate_fn=data_collator,\n",
    "          batch_size=batch_size\n",
    "      )\n",
    "  )\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \"\"\"Cleans and removes special characters from the text.\"\"\"\n",
    "\n",
    "    replacements = [\n",
    "        (r\"what's\", \"what is \"),\n",
    "        (r\"won't\", \"will not \"),\n",
    "        (r\"\\'s\", \" \"),\n",
    "        (r\"\\'ve\", \" have \"),\n",
    "        (r\"can't\", \"can not \"),\n",
    "        (r\"n't\", \" not \"),\n",
    "        (r\"i'm\", \"i am \"),\n",
    "        (r\"\\'re\", \" are \"),\n",
    "        (r\"\\'d\", \" would \"),\n",
    "        (r\"\\'ll\", \" will \"),\n",
    "        (r\"\\'scuse\", \" excuse \"),\n",
    "        (r\"\\'\\n\", \" \"),\n",
    "        (r\"-\", \" \"),\n",
    "        (r\"\\'\\xa0\", \" \"),\n",
    "        (r\"(@.*?)[\\s]\", \" \"),\n",
    "        (r\"&amp;\", \"&\"),\n",
    "    ]\n",
    "    \n",
    "    text = text.lower()\n",
    "    for pattern, replacement in replacements:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6716f726-38e2-4ba8-970d-2a56a0121b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'input', 'label', '__index_level_0__'],\n",
      "    num_rows: 401406\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Kira-Asimov/gender_clinical_trial\", split='train')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3131199-b09d-4d37-a563-11b413a45fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    \"\"\"Tokenises the text and creates a numpy array with its assigned labels.\"\"\"\n",
    "    text = [preprocess_text(text) for text in batch[\"input\"]]\n",
    "    encoding = tokenizer(text, max_length=256, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    labels_batch = batch['label']\n",
    "    #print(labels_batch)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    for idx, temp in enumerate(labels_batch):\n",
    "        #print(label)\n",
    "        label_num= label2id[labels_batch[idx]]\n",
    "        #print(label_num)\n",
    "        labels_matrix[idx, label_num] = 1\n",
    "    #print(labels_matrix)\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    #print(encoding[\"labels\"][0:100])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a014669a-231a-437d-a20d-3bbc603b8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \"\"\"\n",
    "            Custom loss function calculation using BCEWithLogitsLoss, it returns the loss and the outputs if the\n",
    "            return_outputs flag is set to True\n",
    "            This function is used during training, evaluation, and prediction; specifically every time a batch is processed.\n",
    "            The default loss function is here https://github.com/huggingface/transformers/blob/820c46a707ddd033975bc3b0549eea200e64c7da/src/transformers/trainer.py#L2561\n",
    "            \n",
    "            Args:\n",
    "              model: the model we're training\n",
    "              inputs: a dictionary of input tensors\n",
    "              return_outputs: if True, the loss and the model outputs are returned. If False, only the loss is\n",
    "            returned. Defaults to False\n",
    "            \n",
    "            Returns:\n",
    "              The loss and the outputs of the model.\n",
    "            \"\"\"\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            # compute custom loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                            labels.float().view(-1, self.model.config.num_labels))\n",
    "            return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7506f",
   "metadata": {},
   "source": [
    "# Testing the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89cce984-10b1-4cdc-b84e-f54c0bdc79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_classifier(model_name,labels, id2label, label2id):\n",
    "    \"\"\"\n",
    "    We're instantiating a BERT model, and then replacing the classification layer with a custom one for our task.\n",
    "    \n",
    "    Args:\n",
    "      labels: a list of all the labels in the dataset\n",
    "      id2label: a dictionary mapping from label ids to label names\n",
    "      label2id: a dictionary mapping labels to integers\n",
    "    \n",
    "    Returns:\n",
    "      A model with a classifier that has 3 layers.\n",
    "    \"\"\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        problem_type=\"single_label_classification\",\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(768, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, len(labels))\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bfd8b0a-30a8-4a5b-9186-f7cf61f8fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "def training(\n",
    "    epochs= 3 ,\n",
    "    output_model_name= 'gender_',\n",
    "    subset_data: bool = False,\n",
    "    push_to_hub: bool = False,\n",
    "    personal_token: Optional[str] = None,\n",
    "    model_name='domenicrosati/ClinicalTrialBioBert'\n",
    "):\n",
    "    \"\"\"\n",
    "    Main logic of the fine-tuning process: this function loads the dataset, tokenizes it,\n",
    "    splits it into train and validation sets, loads the model, trains it, and saves it\n",
    "    \n",
    "    Args:\n",
    "      epochs (int): number of epochs to train for\n",
    "      output_model_name (str): filename and path to the directory where the model will be saved.\n",
    "      subset_data (bool): flag to indicate whether to use a subset of the data for testing purposes\n",
    "      push_to_hub (bool): flag to indicate whether to push the model to the hub\n",
    "      personal_token (str | None): your personal Hugging Face Hub token\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    dataset_whole=load_dataset(\"Kira-Asimov/gender_clinical_trial\", split='train')\n",
    "    dataset =  dataset_whole.select(range(361265)).train_test_split(test_size=0.1, seed=42)\n",
    "    test_dataset=dataset_whole.select(range(40141))\n",
    "    #print(dataset)\n",
    "    global labels, id2label, label2id\n",
    "    labels, id2label, label2id = get_label_metadata(dataset)\n",
    "    print(labels,id2label, label2id)\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    dataset_cols = [col for col in dataset[\"train\"].column_names if col not in [\"text\", \"input_ids\", \"attention_mask\", \"labels\"]]\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset_cols)\n",
    "    #print(tokenized_dataset)\n",
    "    train_dataset, test_dataset = prepare_splits_for_training(tokenized_dataset, subset_data)\n",
    "    #print(train_dataset)\n",
    "    logging.info(f\"Train dataset length: {len(train_dataset)}\")\n",
    "    logging.info(f\"Test dataset length: {len(test_dataset)}\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_model_name,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        weight_decay=0.01,\n",
    "        data_seed=42,\n",
    "        num_train_epochs=epochs,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        eval_steps=1000,\n",
    "        save_steps=1000\n",
    "        \n",
    "       \n",
    "    )\n",
    "    trainer = MultilabelTrainer(\n",
    "        model=instantiate_classifier(model_name,labels, id2label, label2id),\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(metrics)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    print(predictions)\n",
    "    trainer.save_model(output_model_name)\n",
    "    if push_to_hub:\n",
    "        trainer.push_to_hub()\n",
    "\n",
    "    return trainer,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28d70a3-165d-4268-a5ab-5a17545f4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'None', 'Male', 'Female'] {0: 'All', 1: 'None', 2: 'Male', 3: 'Female'} {'All': 0, 'None': 1, 'Male': 2, 'Female': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0104dd4fc9a54c288db84bc5ea799d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/325138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe386fe28d3a4f609b41f0e41f192741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train dataset length: 325138\n",
      "INFO:root:Test dataset length: 36127\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at domenicrosati/ClinicalTrialBioBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30483' max='30483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30483/30483 3:07:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.146124</td>\n",
       "      <td>0.961442</td>\n",
       "      <td>0.980721</td>\n",
       "      <td>0.961442</td>\n",
       "      <td>0.668738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.136292</td>\n",
       "      <td>0.963628</td>\n",
       "      <td>0.981814</td>\n",
       "      <td>0.963628</td>\n",
       "      <td>0.674584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.127135</td>\n",
       "      <td>0.965012</td>\n",
       "      <td>0.982506</td>\n",
       "      <td>0.965012</td>\n",
       "      <td>0.677971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.125617</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.982935</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.679024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.125586</td>\n",
       "      <td>0.967005</td>\n",
       "      <td>0.983503</td>\n",
       "      <td>0.967005</td>\n",
       "      <td>0.681060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.124586</td>\n",
       "      <td>0.967614</td>\n",
       "      <td>0.983807</td>\n",
       "      <td>0.967614</td>\n",
       "      <td>0.681432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.121664</td>\n",
       "      <td>0.967254</td>\n",
       "      <td>0.983627</td>\n",
       "      <td>0.967254</td>\n",
       "      <td>0.680519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.123526</td>\n",
       "      <td>0.967697</td>\n",
       "      <td>0.983849</td>\n",
       "      <td>0.967697</td>\n",
       "      <td>0.683198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.968611</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.968611</td>\n",
       "      <td>0.685141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.115291</td>\n",
       "      <td>0.969054</td>\n",
       "      <td>0.984527</td>\n",
       "      <td>0.969054</td>\n",
       "      <td>0.685929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.123660</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.985108</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.687434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.116493</td>\n",
       "      <td>0.969663</td>\n",
       "      <td>0.984831</td>\n",
       "      <td>0.969663</td>\n",
       "      <td>0.685959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.121630</td>\n",
       "      <td>0.969303</td>\n",
       "      <td>0.984651</td>\n",
       "      <td>0.969303</td>\n",
       "      <td>0.686363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.116540</td>\n",
       "      <td>0.970161</td>\n",
       "      <td>0.985080</td>\n",
       "      <td>0.970161</td>\n",
       "      <td>0.688779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.116770</td>\n",
       "      <td>0.971240</td>\n",
       "      <td>0.985620</td>\n",
       "      <td>0.971240</td>\n",
       "      <td>0.689792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.985537</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.690260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.118821</td>\n",
       "      <td>0.971047</td>\n",
       "      <td>0.985523</td>\n",
       "      <td>0.971047</td>\n",
       "      <td>0.689867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.112190</td>\n",
       "      <td>0.970189</td>\n",
       "      <td>0.985094</td>\n",
       "      <td>0.970189</td>\n",
       "      <td>0.688723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.115047</td>\n",
       "      <td>0.971573</td>\n",
       "      <td>0.985786</td>\n",
       "      <td>0.971573</td>\n",
       "      <td>0.690094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.110255</td>\n",
       "      <td>0.971379</td>\n",
       "      <td>0.985689</td>\n",
       "      <td>0.971379</td>\n",
       "      <td>0.691295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.112291</td>\n",
       "      <td>0.972071</td>\n",
       "      <td>0.986035</td>\n",
       "      <td>0.972071</td>\n",
       "      <td>0.693190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.972375</td>\n",
       "      <td>0.986188</td>\n",
       "      <td>0.972375</td>\n",
       "      <td>0.693190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.118849</td>\n",
       "      <td>0.972790</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.972790</td>\n",
       "      <td>0.693999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.120279</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.986077</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.692725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>0.115334</td>\n",
       "      <td>0.971379</td>\n",
       "      <td>0.985689</td>\n",
       "      <td>0.971379</td>\n",
       "      <td>0.691204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.122197</td>\n",
       "      <td>0.972514</td>\n",
       "      <td>0.986257</td>\n",
       "      <td>0.972514</td>\n",
       "      <td>0.693286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.119685</td>\n",
       "      <td>0.972846</td>\n",
       "      <td>0.986423</td>\n",
       "      <td>0.972846</td>\n",
       "      <td>0.694442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.118230</td>\n",
       "      <td>0.972431</td>\n",
       "      <td>0.986215</td>\n",
       "      <td>0.972431</td>\n",
       "      <td>0.693868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.117469</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.986354</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.694090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.117440</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.986354</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.694386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory gender/checkpoint-11000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "{'eval_f1': 0.9728457940044842, 'eval_loss': 0.11968476325273514, 'eval_accuracy': 0.9864228970022421, 'eval_f1_micro': 0.9728457940044842, 'eval_f1_macro': 0.694442020880927, 'eval_runtime': 108.9114, 'eval_samples_per_second': 331.71, 'eval_steps_per_second': 10.366, 'epoch': 3.0}\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "PredictionOutput(predictions=array([[ 4.716741  , -2.342588  , -2.5688286 , -0.02610279],\n",
      "       [ 5.1555758 , -1.8308469 , -1.9624968 , -1.0014923 ],\n",
      "       [-1.3350621 , -3.259153  , -3.926163  ,  4.3154197 ],\n",
      "       ...,\n",
      "       [ 5.8448224 , -1.4336758 , -1.792121  , -1.7757436 ],\n",
      "       [ 5.7850986 , -1.3208216 , -1.5124913 , -1.943603  ],\n",
      "       [ 5.435278  , -1.5979738 , -2.031323  , -1.2990754 ]],\n",
      "      dtype=float32), label_ids=array([[1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       ...,\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.11968476325273514, 'test_accuracy': 0.9864228970022421, 'test_f1_micro': 0.9728457940044842, 'test_f1_macro': 0.694442020880927, 'test_eval_f1': 0.9728457940044842, 'test_runtime': 108.2198, 'test_samples_per_second': 333.83, 'test_steps_per_second': 10.432})\n"
     ]
    }
   ],
   "source": [
    "trainer,test_data=training(model_name='domenicrosati/ClinicalTrialBioBert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf34a90-d12c-4978-adfa-571a18b9a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset_whole=load_dataset(\"Kira-Asimov/gender_clinical_trial\", split='train')\n",
    "val_data= dataset_whole.select(range(361265)).train_test_split(test_size=0.1, seed=42)['test']\n",
    "test_data=dataset_whole.select(range(40141))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c0b204-3970-4b2a-8ad6-fa9406c096ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "{'eval_f1': 0.9728457940044842, 'eval_loss': 0.11968476325273514, 'eval_accuracy': 0.9864228970022421, 'eval_f1_micro': 0.9728457940044842, 'eval_f1_macro': 0.694442020880927, 'eval_runtime': 106.9055, 'eval_samples_per_second': 337.934, 'eval_steps_per_second': 10.561, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be0f0bb4-e8e9-48b1-9e1e-2e74c8f51fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6b2bf603f4a47a832097d4dab8f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 ... 0 3 0] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "PredictionOutput(predictions=array([[ 4.9490566, -1.2921993, -0.8475742, -1.8257195],\n",
      "       [ 0.6916529, -3.1520386, -4.12158  ,  3.8424046],\n",
      "       [ 5.7010155, -1.6232574, -2.0388768, -1.4439045],\n",
      "       ...,\n",
      "       [ 5.7971687, -1.3787054, -1.6560345, -1.8532151],\n",
      "       [-1.0265679, -3.062165 , -4.358201 ,  4.1740217],\n",
      "       [ 4.6868644, -1.4692303, -0.2606884, -1.7192413]], dtype=float32), label_ids=array([[1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.059766173362731934, 'test_accuracy': 0.9932737101716449, 'test_f1_micro': 0.9865474203432899, 'test_f1_macro': 0.7243323118107851, 'test_eval_f1': 0.9865474203432899, 'test_runtime': 120.5466, 'test_samples_per_second': 332.991, 'test_steps_per_second': 10.411})\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_data\n",
    "#labels, id2label, label2id = get_label_metadata(test_dataset)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "dataset_cols = [col for col in test_dataset.column_names if col not in [\"text\", \"input_ids\", \"attention_mask\", \"labels\"]]\n",
    "tokenized_test_dataset = test_dataset.map(tokenize, batched=True, remove_columns=dataset_cols)\n",
    "predictions = trainer.predict(tokenized_test_dataset )\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6523a57f-7a14-418f-a42f-a55e2a524160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 ... 0 3 0] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         All     0.9899    0.9952    0.9925     34451\n",
      "        None     0.0000    0.0000    0.0000        58\n",
      "        Male     0.9688    0.9259    0.9469      1646\n",
      "      Female     0.9644    0.9516    0.9579      3986\n",
      "\n",
      "   micro avg     0.9865    0.9865    0.9865     40141\n",
      "   macro avg     0.7308    0.7182    0.7243     40141\n",
      "weighted avg     0.9850    0.9865    0.9858     40141\n",
      " samples avg     0.9865    0.9865    0.9865     40141\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ila/clinical_bio_bert/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize test dataset\n",
    "predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "\n",
    "predicted_labels_index = np.argmax(predictions, axis=-1)\n",
    "num_classes = predictions.shape[1]  # Assuming predictions is a 2D array where axis=1 is the number of classes\n",
    "predicted_labels = np.eye(num_classes)[predicted_labels_index]  \n",
    "true_labels = tokenized_test_dataset[\"labels\"]\n",
    "# Get predictions\n",
    "\n",
    "\n",
    "# Convert probabilities to binary predictions using threshold\n",
    "#predicted_labels = (predictions > sigmoid_threshold).astype(int)\n",
    "report = classification_report(true_labels, predicted_labels, target_names=labels, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a14d5",
   "metadata": {},
   "source": [
    "# Testing Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa7e5337-df73-4165-885f-b4c90d8ecc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel,PeftConfig\n",
    "def instantiate_classifier_finetuned(model_name,labels, id2label, label2id):\n",
    "    \"\"\"\n",
    "    We're instantiating a BERT model, and then replacing the classification layer with a custom one for our task.\n",
    "    \n",
    "    Args:\n",
    "      labels: a list of all the labels in the dataset\n",
    "      id2label: a dictionary mapping from label ids to label names\n",
    "      label2id: a dictionary mapping labels to integers\n",
    "    \n",
    "    Returns:\n",
    "      A model with a classifier that has 3 layers.\n",
    "    \"\"\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       'domenicrosati/ClinicalTrialBioBert',\n",
    "        problem_type=\"single_label_classification\",\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model,model_name,is_trainable=True)\n",
    "    #merged_model=model.merge_and_unload()\n",
    "    #for param in merged_model.parameters():\n",
    "     #    param.requires_grad = True\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(768, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, len(labels))\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def training(\n",
    "    epochs= 3 ,\n",
    "    output_model_name= 'gender_fine-tuned',\n",
    "    subset_data: bool = False,\n",
    "    push_to_hub: bool = False,\n",
    "    personal_token: Optional[str] = None,\n",
    "    model_name='domenicrosati/ClinicalTrialBioBert'\n",
    "):\n",
    "    \"\"\"\n",
    "    Main logic of the fine-tuning process: this function loads the dataset, tokenizes it,\n",
    "    splits it into train and validation sets, loads the model, trains it, and saves it\n",
    "    \n",
    "    Args:\n",
    "      epochs (int): number of epochs to train for\n",
    "      output_model_name (str): filename and path to the directory where the model will be saved.\n",
    "      subset_data (bool): flag to indicate whether to use a subset of the data for testing purposes\n",
    "      push_to_hub (bool): flag to indicate whether to push the model to the hub\n",
    "      personal_token (str | None): your personal Hugging Face Hub token\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    dataset_whole=load_dataset(\"Kira-Asimov/gender_clinical_trial\", split='train')\n",
    "    dataset =  dataset_whole.select(range(361265)).train_test_split(test_size=0.1, seed=42)\n",
    "    test_data=dataset_whole.select(range(40141))\n",
    "    #print(dataset)\n",
    "    global labels, id2label, label2id\n",
    "    labels, id2label, label2id = get_label_metadata(dataset)\n",
    "    print(labels, id2label, label2id)\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('domenicrosati/ClinicalTrialBioBert', do_lower_case=True)\n",
    "    dataset_cols = [col for col in dataset[\"train\"].column_names if col not in [\"text\", \"input_ids\", \"attention_mask\", \"labels\"]]\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset_cols)\n",
    "    #print(tokenized_dataset)\n",
    "    train_dataset, test_dataset = prepare_splits_for_training(tokenized_dataset, subset_data)\n",
    "    #print(train_dataset)\n",
    "    logging.info(f\"Train dataset length: {len(train_dataset)}\")\n",
    "    logging.info(f\"Test dataset length: {len(test_dataset)}\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_model_name,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        weight_decay=0.01,\n",
    "        data_seed=42,\n",
    "        num_train_epochs=epochs,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        save_strategy='no',\n",
    "        eval_steps=1000\n",
    "    )\n",
    "    trainer = MultilabelTrainer(\n",
    "        model= instantiate_classifier_finetuned(model_name,labels, id2label, label2id),\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(metrics)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    print(predictions)\n",
    "    # trainer.save_model(output_model_name)\n",
    "    # if push_to_hub:\n",
    "    #     trainer.push_to_hub()\n",
    "\n",
    "    return trainer,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5b13e69-e676-4a07-a3fc-ab17793df159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'None', 'Male', 'Female'] {0: 'All', 1: 'None', 2: 'Male', 3: 'Female'} {'All': 0, 'None': 1, 'Male': 2, 'Female': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train dataset length: 325138\n",
      "INFO:root:Test dataset length: 36127\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at domenicrosati/ClinicalTrialBioBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30483' max='30483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30483/30483 3:07:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.139390</td>\n",
       "      <td>0.961414</td>\n",
       "      <td>0.980707</td>\n",
       "      <td>0.961414</td>\n",
       "      <td>0.669151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.128615</td>\n",
       "      <td>0.965649</td>\n",
       "      <td>0.982824</td>\n",
       "      <td>0.965649</td>\n",
       "      <td>0.678501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.124494</td>\n",
       "      <td>0.966618</td>\n",
       "      <td>0.983309</td>\n",
       "      <td>0.966618</td>\n",
       "      <td>0.680979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.122136</td>\n",
       "      <td>0.965704</td>\n",
       "      <td>0.982852</td>\n",
       "      <td>0.965704</td>\n",
       "      <td>0.679480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.129894</td>\n",
       "      <td>0.967476</td>\n",
       "      <td>0.983738</td>\n",
       "      <td>0.967476</td>\n",
       "      <td>0.682833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.131053</td>\n",
       "      <td>0.967393</td>\n",
       "      <td>0.983696</td>\n",
       "      <td>0.967393</td>\n",
       "      <td>0.680887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.119495</td>\n",
       "      <td>0.967504</td>\n",
       "      <td>0.983752</td>\n",
       "      <td>0.967504</td>\n",
       "      <td>0.680975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>0.117317</td>\n",
       "      <td>0.967808</td>\n",
       "      <td>0.983904</td>\n",
       "      <td>0.967808</td>\n",
       "      <td>0.683909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.113977</td>\n",
       "      <td>0.969773</td>\n",
       "      <td>0.984887</td>\n",
       "      <td>0.969773</td>\n",
       "      <td>0.687940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.113547</td>\n",
       "      <td>0.969109</td>\n",
       "      <td>0.984554</td>\n",
       "      <td>0.969109</td>\n",
       "      <td>0.685487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.124746</td>\n",
       "      <td>0.970548</td>\n",
       "      <td>0.985274</td>\n",
       "      <td>0.970548</td>\n",
       "      <td>0.688491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.111395</td>\n",
       "      <td>0.970991</td>\n",
       "      <td>0.985496</td>\n",
       "      <td>0.970991</td>\n",
       "      <td>0.688859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.117510</td>\n",
       "      <td>0.971489</td>\n",
       "      <td>0.985745</td>\n",
       "      <td>0.971489</td>\n",
       "      <td>0.690789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.117561</td>\n",
       "      <td>0.970797</td>\n",
       "      <td>0.985399</td>\n",
       "      <td>0.970797</td>\n",
       "      <td>0.690556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.116302</td>\n",
       "      <td>0.971406</td>\n",
       "      <td>0.985703</td>\n",
       "      <td>0.971406</td>\n",
       "      <td>0.690986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.109191</td>\n",
       "      <td>0.971545</td>\n",
       "      <td>0.985772</td>\n",
       "      <td>0.971545</td>\n",
       "      <td>0.690823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.112088</td>\n",
       "      <td>0.971628</td>\n",
       "      <td>0.985814</td>\n",
       "      <td>0.971628</td>\n",
       "      <td>0.691137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.108060</td>\n",
       "      <td>0.971683</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>0.971683</td>\n",
       "      <td>0.691779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.110269</td>\n",
       "      <td>0.972015</td>\n",
       "      <td>0.986008</td>\n",
       "      <td>0.972015</td>\n",
       "      <td>0.692258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.112043</td>\n",
       "      <td>0.972098</td>\n",
       "      <td>0.986049</td>\n",
       "      <td>0.972098</td>\n",
       "      <td>0.692511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.111933</td>\n",
       "      <td>0.972181</td>\n",
       "      <td>0.986091</td>\n",
       "      <td>0.972181</td>\n",
       "      <td>0.693142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.120641</td>\n",
       "      <td>0.972846</td>\n",
       "      <td>0.986423</td>\n",
       "      <td>0.972846</td>\n",
       "      <td>0.694498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.119673</td>\n",
       "      <td>0.972597</td>\n",
       "      <td>0.986298</td>\n",
       "      <td>0.972597</td>\n",
       "      <td>0.694099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.126888</td>\n",
       "      <td>0.971766</td>\n",
       "      <td>0.985883</td>\n",
       "      <td>0.971766</td>\n",
       "      <td>0.692374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.118218</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.986077</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.693122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.123871</td>\n",
       "      <td>0.972763</td>\n",
       "      <td>0.986381</td>\n",
       "      <td>0.972763</td>\n",
       "      <td>0.694083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.120432</td>\n",
       "      <td>0.973040</td>\n",
       "      <td>0.986520</td>\n",
       "      <td>0.973040</td>\n",
       "      <td>0.694614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.119532</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.986354</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.693940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.118626</td>\n",
       "      <td>0.972652</td>\n",
       "      <td>0.986326</td>\n",
       "      <td>0.972652</td>\n",
       "      <td>0.694087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.118403</td>\n",
       "      <td>0.972624</td>\n",
       "      <td>0.986312</td>\n",
       "      <td>0.972624</td>\n",
       "      <td>0.694190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 3 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 3 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "{'eval_f1': 0.9729011542613558, 'eval_loss': 0.119154192507267, 'eval_accuracy': 0.9864505771306779, 'eval_f1_micro': 0.9729011542613558, 'eval_f1_macro': 0.6947800023735957, 'eval_runtime': 110.4846, 'eval_samples_per_second': 326.987, 'eval_steps_per_second': 10.219, 'epoch': 3.0}\n",
      "[0 0 3 ... 0 0 0] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "PredictionOutput(predictions=array([[ 4.771084  , -2.0946858 , -2.7640128 , -0.06948467],\n",
      "       [ 4.0393047 , -2.8971076 , -0.9918452 , -0.6479542 ],\n",
      "       [-0.8695626 , -1.7980453 , -3.0349772 ,  5.308833  ],\n",
      "       ...,\n",
      "       [ 5.78096   , -1.4707508 , -2.7121763 , -1.7542156 ],\n",
      "       [ 5.43031   , -1.5971072 , -2.8763437 , -1.1890135 ],\n",
      "       [ 5.101243  , -1.9735339 , -3.0895972 , -0.24051614]],\n",
      "      dtype=float32), label_ids=array([[1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       ...,\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.119154192507267, 'test_accuracy': 0.9864505771306779, 'test_f1_micro': 0.9729011542613558, 'test_f1_macro': 0.6947800023735957, 'test_eval_f1': 0.9729011542613558, 'test_runtime': 110.1814, 'test_samples_per_second': 327.887, 'test_steps_per_second': 10.247})\n"
     ]
    }
   ],
   "source": [
    "trainer_new,test_data_new=training(model_name='checkpoint-205835')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4da71dca-68c3-4b2e-bfe5-a55c01de120d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767711658c34d879c732e644f22e7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 ... 0 3 0] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "PredictionOutput(predictions=array([[ 4.282083 , -2.168684 , -0.6400821, -1.9821142],\n",
      "       [ 1.6935655, -2.6631646, -3.765895 ,  4.2597303],\n",
      "       [ 5.553697 , -1.994703 , -2.4438477, -1.3117491],\n",
      "       ...,\n",
      "       [ 5.7561235, -1.431405 , -2.3210175, -2.203385 ],\n",
      "       [-0.6470486, -1.8412061, -3.2235136,  5.3283615],\n",
      "       [ 5.581014 , -1.4580578, -2.3946776, -1.894033 ]], dtype=float32), label_ids=array([[1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.05281538888812065, 'test_accuracy': 0.994245285369074, 'test_f1_micro': 0.9884905707381481, 'test_f1_macro': 0.728208965271378, 'test_eval_f1': 0.9884905707381481, 'test_runtime': 122.6876, 'test_samples_per_second': 327.181, 'test_steps_per_second': 10.229})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_dataset = test_data_new\n",
    "\n",
    "# Tokenize test dataset\n",
    "\n",
    "dataset_cols = [col for col in test_dataset.column_names if col not in [\"text\", \"input_ids\", \"attention_mask\", \"labels\"]]\n",
    "tokenized_test_dataset = test_dataset.map(tokenize, batched=True, remove_columns=dataset_cols)\n",
    "predictions = trainer_new.predict(tokenized_test_dataset )\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7aa34ebc-078d-4ea5-8f5f-191630cdb280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 ... 0 3 0] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         All     0.9911    0.9960    0.9935     34451\n",
      "        None     0.0000    0.0000    0.0000        58\n",
      "        Male     0.9722    0.9350    0.9532      1646\n",
      "      Female     0.9718    0.9604    0.9661      3986\n",
      "\n",
      "   micro avg     0.9885    0.9885    0.9885     40141\n",
      "   macro avg     0.7338    0.7228    0.7282     40141\n",
      "weighted avg     0.9870    0.9885    0.9877     40141\n",
      " samples avg     0.9885    0.9885    0.9885     40141\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ila/clinical_bio_bert/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize test dataset\n",
    "predictions = trainer_new.predict(tokenized_test_dataset).predictions\n",
    "\n",
    "predicted_labels_index = np.argmax(predictions, axis=-1)\n",
    "num_classes = predictions.shape[1]  # Assuming predictions is a 2D array where axis=1 is the number of classes\n",
    "predicted_labels = np.eye(num_classes)[predicted_labels_index]  \n",
    "true_labels = tokenized_test_dataset[\"labels\"]\n",
    "# Get predictions\n",
    "\n",
    "\n",
    "# Convert probabilities to binary predictions using threshold\n",
    "#predicted_labels = (predictions > sigmoid_threshold).astype(int)\n",
    "report = classification_report(true_labels, predicted_labels, target_names=labels, digits=4)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
