{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 205835,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 2.4897637367248535,
      "learning_rate": 2.992712609614497e-06,
      "loss": 0.8427,
      "step": 500
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.043006420135498,
      "learning_rate": 2.985425219228994e-06,
      "loss": 0.8453,
      "step": 1000
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.0849945545196533,
      "learning_rate": 2.978137828843491e-06,
      "loss": 0.8461,
      "step": 1500
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.71857488155365,
      "learning_rate": 2.9708504384579884e-06,
      "loss": 0.8549,
      "step": 2000
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.613085150718689,
      "learning_rate": 2.9635630480724854e-06,
      "loss": 0.8524,
      "step": 2500
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.6222506761550903,
      "learning_rate": 2.9562756576869823e-06,
      "loss": 0.8536,
      "step": 3000
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.816859483718872,
      "learning_rate": 2.9489882673014793e-06,
      "loss": 0.8531,
      "step": 3500
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.6279484033584595,
      "learning_rate": 2.9417008769159763e-06,
      "loss": 0.849,
      "step": 4000
    },
    {
      "epoch": 0.11,
      "grad_norm": 2.3566184043884277,
      "learning_rate": 2.9344134865304733e-06,
      "loss": 0.8409,
      "step": 4500
    },
    {
      "epoch": 0.12,
      "grad_norm": 2.383244037628174,
      "learning_rate": 2.9271260961449702e-06,
      "loss": 0.8419,
      "step": 5000
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.6024855375289917,
      "learning_rate": 2.9198387057594676e-06,
      "loss": 0.8464,
      "step": 5500
    },
    {
      "epoch": 0.15,
      "grad_norm": 2.3701140880584717,
      "learning_rate": 2.9125513153739646e-06,
      "loss": 0.8476,
      "step": 6000
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.9495985507965088,
      "learning_rate": 2.905263924988462e-06,
      "loss": 0.8508,
      "step": 6500
    },
    {
      "epoch": 0.17,
      "grad_norm": 1.7550445795059204,
      "learning_rate": 2.897976534602959e-06,
      "loss": 0.859,
      "step": 7000
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.8080904483795166,
      "learning_rate": 2.890689144217456e-06,
      "loss": 0.8509,
      "step": 7500
    },
    {
      "epoch": 0.19,
      "grad_norm": 1.6375343799591064,
      "learning_rate": 2.883401753831953e-06,
      "loss": 0.8392,
      "step": 8000
    },
    {
      "epoch": 0.21,
      "grad_norm": 1.8706011772155762,
      "learning_rate": 2.87611436344645e-06,
      "loss": 0.8528,
      "step": 8500
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.3451288938522339,
      "learning_rate": 2.868826973060947e-06,
      "loss": 0.84,
      "step": 9000
    },
    {
      "epoch": 0.23,
      "grad_norm": 2.22172474861145,
      "learning_rate": 2.861539582675444e-06,
      "loss": 0.8557,
      "step": 9500
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.5868152379989624,
      "learning_rate": 2.8542521922899412e-06,
      "loss": 0.859,
      "step": 10000
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.983375906944275,
      "learning_rate": 2.846964801904438e-06,
      "loss": 0.8561,
      "step": 10500
    },
    {
      "epoch": 0.27,
      "grad_norm": 1.6845961809158325,
      "learning_rate": 2.839677411518935e-06,
      "loss": 0.843,
      "step": 11000
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.1979892253875732,
      "learning_rate": 2.832390021133432e-06,
      "loss": 0.859,
      "step": 11500
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.6375226974487305,
      "learning_rate": 2.825102630747929e-06,
      "loss": 0.8626,
      "step": 12000
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.869890570640564,
      "learning_rate": 2.817815240362426e-06,
      "loss": 0.8407,
      "step": 12500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.747048020362854,
      "learning_rate": 2.810527849976923e-06,
      "loss": 0.8478,
      "step": 13000
    },
    {
      "epoch": 0.33,
      "grad_norm": 2.117492198944092,
      "learning_rate": 2.8032404595914205e-06,
      "loss": 0.848,
      "step": 13500
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.3532085418701172,
      "learning_rate": 2.7959530692059174e-06,
      "loss": 0.8482,
      "step": 14000
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.7377387285232544,
      "learning_rate": 2.7886656788204144e-06,
      "loss": 0.8495,
      "step": 14500
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.0943753719329834,
      "learning_rate": 2.7813782884349114e-06,
      "loss": 0.847,
      "step": 15000
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.8466728925704956,
      "learning_rate": 2.7740908980494088e-06,
      "loss": 0.8491,
      "step": 15500
    },
    {
      "epoch": 0.39,
      "grad_norm": 1.6191214323043823,
      "learning_rate": 2.7668035076639057e-06,
      "loss": 0.8499,
      "step": 16000
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.2432503700256348,
      "learning_rate": 2.7595161172784027e-06,
      "loss": 0.8523,
      "step": 16500
    },
    {
      "epoch": 0.41,
      "grad_norm": 1.4978612661361694,
      "learning_rate": 2.7522287268928997e-06,
      "loss": 0.8479,
      "step": 17000
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.9574241638183594,
      "learning_rate": 2.744941336507397e-06,
      "loss": 0.8438,
      "step": 17500
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.6164367198944092,
      "learning_rate": 2.737653946121894e-06,
      "loss": 0.8462,
      "step": 18000
    },
    {
      "epoch": 0.45,
      "grad_norm": 1.6198246479034424,
      "learning_rate": 2.730366555736391e-06,
      "loss": 0.8428,
      "step": 18500
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.9119200706481934,
      "learning_rate": 2.723079165350888e-06,
      "loss": 0.838,
      "step": 19000
    },
    {
      "epoch": 0.47,
      "grad_norm": 1.9040404558181763,
      "learning_rate": 2.715791774965385e-06,
      "loss": 0.8476,
      "step": 19500
    },
    {
      "epoch": 0.49,
      "grad_norm": 1.8511701822280884,
      "learning_rate": 2.708504384579882e-06,
      "loss": 0.8492,
      "step": 20000
    },
    {
      "epoch": 0.5,
      "grad_norm": 2.075007915496826,
      "learning_rate": 2.701216994194379e-06,
      "loss": 0.8487,
      "step": 20500
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.3555527925491333,
      "learning_rate": 2.693929603808876e-06,
      "loss": 0.8368,
      "step": 21000
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.7003114223480225,
      "learning_rate": 2.6866422134233733e-06,
      "loss": 0.8297,
      "step": 21500
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.0258499383926392,
      "learning_rate": 2.6793548230378703e-06,
      "loss": 0.8596,
      "step": 22000
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.6509355306625366,
      "learning_rate": 2.6720674326523672e-06,
      "loss": 0.8397,
      "step": 22500
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.7327628135681152,
      "learning_rate": 2.664780042266864e-06,
      "loss": 0.8567,
      "step": 23000
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.7404568195343018,
      "learning_rate": 2.657492651881361e-06,
      "loss": 0.8515,
      "step": 23500
    },
    {
      "epoch": 0.58,
      "grad_norm": 2.003720760345459,
      "learning_rate": 2.650205261495858e-06,
      "loss": 0.8634,
      "step": 24000
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.6466221809387207,
      "learning_rate": 2.642917871110355e-06,
      "loss": 0.8449,
      "step": 24500
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.5129305124282837,
      "learning_rate": 2.6356304807248525e-06,
      "loss": 0.8427,
      "step": 25000
    },
    {
      "epoch": 0.62,
      "grad_norm": 2.700401782989502,
      "learning_rate": 2.6283430903393495e-06,
      "loss": 0.8491,
      "step": 25500
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.847428798675537,
      "learning_rate": 2.621055699953847e-06,
      "loss": 0.8509,
      "step": 26000
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.7392017841339111,
      "learning_rate": 2.613768309568344e-06,
      "loss": 0.8456,
      "step": 26500
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.9331609010696411,
      "learning_rate": 2.606480919182841e-06,
      "loss": 0.8498,
      "step": 27000
    },
    {
      "epoch": 0.67,
      "grad_norm": 1.4420933723449707,
      "learning_rate": 2.599193528797338e-06,
      "loss": 0.8336,
      "step": 27500
    },
    {
      "epoch": 0.68,
      "grad_norm": 2.231842041015625,
      "learning_rate": 2.5919061384118348e-06,
      "loss": 0.854,
      "step": 28000
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.6373875141143799,
      "learning_rate": 2.5846187480263317e-06,
      "loss": 0.8449,
      "step": 28500
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.982932686805725,
      "learning_rate": 2.577331357640829e-06,
      "loss": 0.8449,
      "step": 29000
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.6367294788360596,
      "learning_rate": 2.570043967255326e-06,
      "loss": 0.8479,
      "step": 29500
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.7645204067230225,
      "learning_rate": 2.562756576869823e-06,
      "loss": 0.8373,
      "step": 30000
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.206462025642395,
      "learning_rate": 2.55546918648432e-06,
      "loss": 0.8358,
      "step": 30500
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.0578196048736572,
      "learning_rate": 2.548181796098817e-06,
      "loss": 0.8528,
      "step": 31000
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.9620603322982788,
      "learning_rate": 2.540894405713314e-06,
      "loss": 0.8351,
      "step": 31500
    },
    {
      "epoch": 0.78,
      "grad_norm": 2.0557854175567627,
      "learning_rate": 2.533607015327811e-06,
      "loss": 0.8425,
      "step": 32000
    },
    {
      "epoch": 0.79,
      "grad_norm": 1.9734314680099487,
      "learning_rate": 2.5263196249423084e-06,
      "loss": 0.8469,
      "step": 32500
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.9906443357467651,
      "learning_rate": 2.5190322345568053e-06,
      "loss": 0.8421,
      "step": 33000
    },
    {
      "epoch": 0.81,
      "grad_norm": 1.8147066831588745,
      "learning_rate": 2.5117448441713023e-06,
      "loss": 0.8503,
      "step": 33500
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.8633458614349365,
      "learning_rate": 2.5044574537857993e-06,
      "loss": 0.8525,
      "step": 34000
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.399416208267212,
      "learning_rate": 2.4971700634002963e-06,
      "loss": 0.8535,
      "step": 34500
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.472579002380371,
      "learning_rate": 2.4898826730147932e-06,
      "loss": 0.8458,
      "step": 35000
    },
    {
      "epoch": 0.86,
      "grad_norm": 2.107666254043579,
      "learning_rate": 2.48259528262929e-06,
      "loss": 0.8565,
      "step": 35500
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.7060712575912476,
      "learning_rate": 2.4753078922437876e-06,
      "loss": 0.8304,
      "step": 36000
    },
    {
      "epoch": 0.89,
      "grad_norm": 2.423685312271118,
      "learning_rate": 2.4680205018582846e-06,
      "loss": 0.8567,
      "step": 36500
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.1041507720947266,
      "learning_rate": 2.460733111472782e-06,
      "loss": 0.8408,
      "step": 37000
    },
    {
      "epoch": 0.91,
      "grad_norm": 1.947846531867981,
      "learning_rate": 2.453445721087279e-06,
      "loss": 0.8548,
      "step": 37500
    },
    {
      "epoch": 0.92,
      "grad_norm": 2.3253767490386963,
      "learning_rate": 2.446158330701776e-06,
      "loss": 0.8513,
      "step": 38000
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.299868106842041,
      "learning_rate": 2.438870940316273e-06,
      "loss": 0.8453,
      "step": 38500
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.704152226448059,
      "learning_rate": 2.43158354993077e-06,
      "loss": 0.8385,
      "step": 39000
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.42793607711792,
      "learning_rate": 2.424296159545267e-06,
      "loss": 0.8299,
      "step": 39500
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.7405097484588623,
      "learning_rate": 2.417008769159764e-06,
      "loss": 0.8504,
      "step": 40000
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.3810043334960938,
      "learning_rate": 2.409721378774261e-06,
      "loss": 0.843,
      "step": 40500
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.5388469696044922,
      "learning_rate": 2.402433988388758e-06,
      "loss": 0.8417,
      "step": 41000
    },
    {
      "epoch": 1.01,
      "grad_norm": 1.734786033630371,
      "learning_rate": 2.395146598003255e-06,
      "loss": 0.8359,
      "step": 41500
    },
    {
      "epoch": 1.02,
      "grad_norm": 2.905111074447632,
      "learning_rate": 2.387859207617752e-06,
      "loss": 0.8466,
      "step": 42000
    },
    {
      "epoch": 1.03,
      "grad_norm": 1.6879557371139526,
      "learning_rate": 2.380571817232249e-06,
      "loss": 0.8418,
      "step": 42500
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.8045716285705566,
      "learning_rate": 2.373284426846746e-06,
      "loss": 0.8439,
      "step": 43000
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.3648810386657715,
      "learning_rate": 2.365997036461243e-06,
      "loss": 0.8414,
      "step": 43500
    },
    {
      "epoch": 1.07,
      "grad_norm": 1.7180161476135254,
      "learning_rate": 2.3587096460757404e-06,
      "loss": 0.8509,
      "step": 44000
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.808119058609009,
      "learning_rate": 2.3514222556902374e-06,
      "loss": 0.8462,
      "step": 44500
    },
    {
      "epoch": 1.09,
      "grad_norm": 2.086043357849121,
      "learning_rate": 2.3441348653047344e-06,
      "loss": 0.8481,
      "step": 45000
    },
    {
      "epoch": 1.11,
      "grad_norm": 1.6525799036026,
      "learning_rate": 2.3368474749192313e-06,
      "loss": 0.8458,
      "step": 45500
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.2666022777557373,
      "learning_rate": 2.3295600845337283e-06,
      "loss": 0.8458,
      "step": 46000
    },
    {
      "epoch": 1.13,
      "grad_norm": 2.5138142108917236,
      "learning_rate": 2.3222726941482257e-06,
      "loss": 0.8467,
      "step": 46500
    },
    {
      "epoch": 1.14,
      "grad_norm": 1.8316065073013306,
      "learning_rate": 2.3149853037627227e-06,
      "loss": 0.8538,
      "step": 47000
    },
    {
      "epoch": 1.15,
      "grad_norm": 2.171718120574951,
      "learning_rate": 2.3076979133772197e-06,
      "loss": 0.8438,
      "step": 47500
    },
    {
      "epoch": 1.17,
      "grad_norm": 1.7910816669464111,
      "learning_rate": 2.3004105229917166e-06,
      "loss": 0.8569,
      "step": 48000
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.301667332649231,
      "learning_rate": 2.293123132606214e-06,
      "loss": 0.8386,
      "step": 48500
    },
    {
      "epoch": 1.19,
      "grad_norm": 2.1113641262054443,
      "learning_rate": 2.285835742220711e-06,
      "loss": 0.8528,
      "step": 49000
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.476844072341919,
      "learning_rate": 2.278548351835208e-06,
      "loss": 0.8354,
      "step": 49500
    },
    {
      "epoch": 1.21,
      "grad_norm": 1.4945358037948608,
      "learning_rate": 2.271260961449705e-06,
      "loss": 0.8445,
      "step": 50000
    },
    {
      "epoch": 1.23,
      "grad_norm": 1.2285089492797852,
      "learning_rate": 2.263973571064202e-06,
      "loss": 0.8362,
      "step": 50500
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.9457049369812012,
      "learning_rate": 2.256686180678699e-06,
      "loss": 0.8536,
      "step": 51000
    },
    {
      "epoch": 1.25,
      "grad_norm": 1.7104990482330322,
      "learning_rate": 2.249398790293196e-06,
      "loss": 0.8389,
      "step": 51500
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.9253513813018799,
      "learning_rate": 2.2421113999076933e-06,
      "loss": 0.8546,
      "step": 52000
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.840947151184082,
      "learning_rate": 2.2348240095221902e-06,
      "loss": 0.8345,
      "step": 52500
    },
    {
      "epoch": 1.29,
      "grad_norm": 1.7903136014938354,
      "learning_rate": 2.227536619136687e-06,
      "loss": 0.8478,
      "step": 53000
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.6060911417007446,
      "learning_rate": 2.220249228751184e-06,
      "loss": 0.84,
      "step": 53500
    },
    {
      "epoch": 1.31,
      "grad_norm": 1.3766034841537476,
      "learning_rate": 2.212961838365681e-06,
      "loss": 0.8532,
      "step": 54000
    },
    {
      "epoch": 1.32,
      "grad_norm": 2.012467622756958,
      "learning_rate": 2.205674447980178e-06,
      "loss": 0.8445,
      "step": 54500
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.895300030708313,
      "learning_rate": 2.198387057594675e-06,
      "loss": 0.8488,
      "step": 55000
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.7633159160614014,
      "learning_rate": 2.1910996672091725e-06,
      "loss": 0.8323,
      "step": 55500
    },
    {
      "epoch": 1.36,
      "grad_norm": 1.7506155967712402,
      "learning_rate": 2.1838122768236695e-06,
      "loss": 0.856,
      "step": 56000
    },
    {
      "epoch": 1.37,
      "grad_norm": 2.114403009414673,
      "learning_rate": 2.1765248864381664e-06,
      "loss": 0.8656,
      "step": 56500
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.4602837562561035,
      "learning_rate": 2.169237496052664e-06,
      "loss": 0.8607,
      "step": 57000
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.143101215362549,
      "learning_rate": 2.161950105667161e-06,
      "loss": 0.8342,
      "step": 57500
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.3036179542541504,
      "learning_rate": 2.1546627152816578e-06,
      "loss": 0.8644,
      "step": 58000
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.6135975122451782,
      "learning_rate": 2.1473753248961548e-06,
      "loss": 0.8261,
      "step": 58500
    },
    {
      "epoch": 1.43,
      "grad_norm": 1.8490052223205566,
      "learning_rate": 2.1400879345106517e-06,
      "loss": 0.8365,
      "step": 59000
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.3686603307724,
      "learning_rate": 2.132800544125149e-06,
      "loss": 0.8422,
      "step": 59500
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.5080264806747437,
      "learning_rate": 2.125513153739646e-06,
      "loss": 0.8475,
      "step": 60000
    },
    {
      "epoch": 1.47,
      "grad_norm": 1.2761484384536743,
      "learning_rate": 2.118225763354143e-06,
      "loss": 0.8534,
      "step": 60500
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.896786093711853,
      "learning_rate": 2.11093837296864e-06,
      "loss": 0.8496,
      "step": 61000
    },
    {
      "epoch": 1.49,
      "grad_norm": 2.1872658729553223,
      "learning_rate": 2.103650982583137e-06,
      "loss": 0.8436,
      "step": 61500
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.6214122772216797,
      "learning_rate": 2.096363592197634e-06,
      "loss": 0.8496,
      "step": 62000
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.3249647617340088,
      "learning_rate": 2.089076201812131e-06,
      "loss": 0.8488,
      "step": 62500
    },
    {
      "epoch": 1.53,
      "grad_norm": 1.4655475616455078,
      "learning_rate": 2.081788811426628e-06,
      "loss": 0.8342,
      "step": 63000
    },
    {
      "epoch": 1.54,
      "grad_norm": 2.209181547164917,
      "learning_rate": 2.0745014210411253e-06,
      "loss": 0.8415,
      "step": 63500
    },
    {
      "epoch": 1.55,
      "grad_norm": 2.400714874267578,
      "learning_rate": 2.0672140306556223e-06,
      "loss": 0.8381,
      "step": 64000
    },
    {
      "epoch": 1.57,
      "grad_norm": 1.442524790763855,
      "learning_rate": 2.0599266402701193e-06,
      "loss": 0.8406,
      "step": 64500
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.7944552898406982,
      "learning_rate": 2.0526392498846162e-06,
      "loss": 0.8569,
      "step": 65000
    },
    {
      "epoch": 1.59,
      "grad_norm": 1.3234699964523315,
      "learning_rate": 2.0453518594991132e-06,
      "loss": 0.8661,
      "step": 65500
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.520745038986206,
      "learning_rate": 2.03806446911361e-06,
      "loss": 0.847,
      "step": 66000
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.5543317794799805,
      "learning_rate": 2.030777078728107e-06,
      "loss": 0.85,
      "step": 66500
    },
    {
      "epoch": 1.63,
      "grad_norm": 2.1693427562713623,
      "learning_rate": 2.0234896883426046e-06,
      "loss": 0.8519,
      "step": 67000
    },
    {
      "epoch": 1.64,
      "grad_norm": 1.948686122894287,
      "learning_rate": 2.016202297957102e-06,
      "loss": 0.8496,
      "step": 67500
    },
    {
      "epoch": 1.65,
      "grad_norm": 2.314094066619873,
      "learning_rate": 2.008914907571599e-06,
      "loss": 0.8457,
      "step": 68000
    },
    {
      "epoch": 1.66,
      "grad_norm": 1.5317955017089844,
      "learning_rate": 2.001627517186096e-06,
      "loss": 0.838,
      "step": 68500
    },
    {
      "epoch": 1.68,
      "grad_norm": 1.6393285989761353,
      "learning_rate": 1.994340126800593e-06,
      "loss": 0.8457,
      "step": 69000
    },
    {
      "epoch": 1.69,
      "grad_norm": 2.1035313606262207,
      "learning_rate": 1.98705273641509e-06,
      "loss": 0.8477,
      "step": 69500
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.3924832344055176,
      "learning_rate": 1.979765346029587e-06,
      "loss": 0.8528,
      "step": 70000
    },
    {
      "epoch": 1.71,
      "grad_norm": 1.636717438697815,
      "learning_rate": 1.9724779556440838e-06,
      "loss": 0.8467,
      "step": 70500
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.6697698831558228,
      "learning_rate": 1.965190565258581e-06,
      "loss": 0.8378,
      "step": 71000
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.2966989278793335,
      "learning_rate": 1.957903174873078e-06,
      "loss": 0.8411,
      "step": 71500
    },
    {
      "epoch": 1.75,
      "grad_norm": 2.579376459121704,
      "learning_rate": 1.950615784487575e-06,
      "loss": 0.8479,
      "step": 72000
    },
    {
      "epoch": 1.76,
      "grad_norm": 2.086101531982422,
      "learning_rate": 1.943328394102072e-06,
      "loss": 0.8533,
      "step": 72500
    },
    {
      "epoch": 1.77,
      "grad_norm": 1.5239472389221191,
      "learning_rate": 1.936041003716569e-06,
      "loss": 0.851,
      "step": 73000
    },
    {
      "epoch": 1.79,
      "grad_norm": 2.0362837314605713,
      "learning_rate": 1.928753613331066e-06,
      "loss": 0.8412,
      "step": 73500
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.294822096824646,
      "learning_rate": 1.921466222945563e-06,
      "loss": 0.8447,
      "step": 74000
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.4747904539108276,
      "learning_rate": 1.9141788325600604e-06,
      "loss": 0.8507,
      "step": 74500
    },
    {
      "epoch": 1.82,
      "grad_norm": 1.3856145143508911,
      "learning_rate": 1.9068914421745572e-06,
      "loss": 0.8331,
      "step": 75000
    },
    {
      "epoch": 1.83,
      "grad_norm": 1.5759600400924683,
      "learning_rate": 1.8996040517890544e-06,
      "loss": 0.852,
      "step": 75500
    },
    {
      "epoch": 1.85,
      "grad_norm": 1.6420427560806274,
      "learning_rate": 1.8923166614035513e-06,
      "loss": 0.8361,
      "step": 76000
    },
    {
      "epoch": 1.86,
      "grad_norm": 3.5820627212524414,
      "learning_rate": 1.8850292710180483e-06,
      "loss": 0.8499,
      "step": 76500
    },
    {
      "epoch": 1.87,
      "grad_norm": 1.703369140625,
      "learning_rate": 1.8777418806325453e-06,
      "loss": 0.8458,
      "step": 77000
    },
    {
      "epoch": 1.88,
      "grad_norm": 2.486633539199829,
      "learning_rate": 1.8704544902470427e-06,
      "loss": 0.8538,
      "step": 77500
    },
    {
      "epoch": 1.89,
      "grad_norm": 1.8514484167099,
      "learning_rate": 1.8631670998615399e-06,
      "loss": 0.8439,
      "step": 78000
    },
    {
      "epoch": 1.91,
      "grad_norm": 1.5104950666427612,
      "learning_rate": 1.8558797094760368e-06,
      "loss": 0.8449,
      "step": 78500
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.773410677909851,
      "learning_rate": 1.8485923190905338e-06,
      "loss": 0.844,
      "step": 79000
    },
    {
      "epoch": 1.93,
      "grad_norm": 1.5803155899047852,
      "learning_rate": 1.8413049287050308e-06,
      "loss": 0.8458,
      "step": 79500
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.7947314977645874,
      "learning_rate": 1.834017538319528e-06,
      "loss": 0.836,
      "step": 80000
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.5883265733718872,
      "learning_rate": 1.826730147934025e-06,
      "loss": 0.8385,
      "step": 80500
    },
    {
      "epoch": 1.97,
      "grad_norm": 2.2216877937316895,
      "learning_rate": 1.819442757548522e-06,
      "loss": 0.8469,
      "step": 81000
    },
    {
      "epoch": 1.98,
      "grad_norm": 2.7281405925750732,
      "learning_rate": 1.812155367163019e-06,
      "loss": 0.8392,
      "step": 81500
    },
    {
      "epoch": 1.99,
      "grad_norm": 1.2615376710891724,
      "learning_rate": 1.804867976777516e-06,
      "loss": 0.8467,
      "step": 82000
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3871760368347168,
      "learning_rate": 1.797580586392013e-06,
      "loss": 0.8308,
      "step": 82500
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.9553722143173218,
      "learning_rate": 1.79029319600651e-06,
      "loss": 0.8429,
      "step": 83000
    },
    {
      "epoch": 2.03,
      "grad_norm": 1.772788643836975,
      "learning_rate": 1.7830058056210072e-06,
      "loss": 0.8365,
      "step": 83500
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.7437946796417236,
      "learning_rate": 1.7757184152355042e-06,
      "loss": 0.8378,
      "step": 84000
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.7333290576934814,
      "learning_rate": 1.7684310248500011e-06,
      "loss": 0.8566,
      "step": 84500
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.907484531402588,
      "learning_rate": 1.7611436344644983e-06,
      "loss": 0.8497,
      "step": 85000
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.4185224771499634,
      "learning_rate": 1.7538562440789953e-06,
      "loss": 0.8403,
      "step": 85500
    },
    {
      "epoch": 2.09,
      "grad_norm": 1.5406520366668701,
      "learning_rate": 1.7465688536934923e-06,
      "loss": 0.8452,
      "step": 86000
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.7029961347579956,
      "learning_rate": 1.7392814633079892e-06,
      "loss": 0.8362,
      "step": 86500
    },
    {
      "epoch": 2.11,
      "grad_norm": 1.5579028129577637,
      "learning_rate": 1.7319940729224864e-06,
      "loss": 0.8435,
      "step": 87000
    },
    {
      "epoch": 2.13,
      "grad_norm": 1.120592713356018,
      "learning_rate": 1.7247066825369838e-06,
      "loss": 0.8423,
      "step": 87500
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.6069221496582031,
      "learning_rate": 1.7174192921514808e-06,
      "loss": 0.8555,
      "step": 88000
    },
    {
      "epoch": 2.15,
      "grad_norm": 1.482855200767517,
      "learning_rate": 1.7101319017659778e-06,
      "loss": 0.8448,
      "step": 88500
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.10996413230896,
      "learning_rate": 1.7028445113804747e-06,
      "loss": 0.8421,
      "step": 89000
    },
    {
      "epoch": 2.17,
      "grad_norm": 1.6909745931625366,
      "learning_rate": 1.695557120994972e-06,
      "loss": 0.838,
      "step": 89500
    },
    {
      "epoch": 2.19,
      "grad_norm": 1.6927716732025146,
      "learning_rate": 1.6882697306094689e-06,
      "loss": 0.8369,
      "step": 90000
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.716308355331421,
      "learning_rate": 1.6809823402239659e-06,
      "loss": 0.847,
      "step": 90500
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.6517796516418457,
      "learning_rate": 1.673694949838463e-06,
      "loss": 0.8316,
      "step": 91000
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.93185555934906,
      "learning_rate": 1.66640755945296e-06,
      "loss": 0.8497,
      "step": 91500
    },
    {
      "epoch": 2.23,
      "grad_norm": 1.365523099899292,
      "learning_rate": 1.659120169067457e-06,
      "loss": 0.8502,
      "step": 92000
    },
    {
      "epoch": 2.25,
      "grad_norm": 2.5079476833343506,
      "learning_rate": 1.651832778681954e-06,
      "loss": 0.8424,
      "step": 92500
    },
    {
      "epoch": 2.26,
      "grad_norm": 2.2969510555267334,
      "learning_rate": 1.6445453882964511e-06,
      "loss": 0.8487,
      "step": 93000
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.8348383903503418,
      "learning_rate": 1.6372579979109481e-06,
      "loss": 0.8405,
      "step": 93500
    },
    {
      "epoch": 2.28,
      "grad_norm": 2.1381828784942627,
      "learning_rate": 1.629970607525445e-06,
      "loss": 0.8491,
      "step": 94000
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.9148272275924683,
      "learning_rate": 1.622683217139942e-06,
      "loss": 0.8517,
      "step": 94500
    },
    {
      "epoch": 2.31,
      "grad_norm": 1.6178178787231445,
      "learning_rate": 1.6153958267544392e-06,
      "loss": 0.8404,
      "step": 95000
    },
    {
      "epoch": 2.32,
      "grad_norm": 2.3397669792175293,
      "learning_rate": 1.6081084363689362e-06,
      "loss": 0.8539,
      "step": 95500
    },
    {
      "epoch": 2.33,
      "grad_norm": 2.205869674682617,
      "learning_rate": 1.6008210459834332e-06,
      "loss": 0.8307,
      "step": 96000
    },
    {
      "epoch": 2.34,
      "grad_norm": 2.6355087757110596,
      "learning_rate": 1.5935336555979304e-06,
      "loss": 0.8465,
      "step": 96500
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.7431657314300537,
      "learning_rate": 1.5862462652124274e-06,
      "loss": 0.8355,
      "step": 97000
    },
    {
      "epoch": 2.37,
      "grad_norm": 2.5574159622192383,
      "learning_rate": 1.5789588748269243e-06,
      "loss": 0.8395,
      "step": 97500
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.7554538249969482,
      "learning_rate": 1.5716714844414217e-06,
      "loss": 0.8407,
      "step": 98000
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.7532058954238892,
      "learning_rate": 1.5643840940559187e-06,
      "loss": 0.8333,
      "step": 98500
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.3058303594589233,
      "learning_rate": 1.5570967036704159e-06,
      "loss": 0.8426,
      "step": 99000
    },
    {
      "epoch": 2.42,
      "grad_norm": 2.213650703430176,
      "learning_rate": 1.5498093132849128e-06,
      "loss": 0.8347,
      "step": 99500
    },
    {
      "epoch": 2.43,
      "grad_norm": 1.6357070207595825,
      "learning_rate": 1.5425219228994098e-06,
      "loss": 0.84,
      "step": 100000
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.2437740564346313,
      "learning_rate": 1.5352345325139068e-06,
      "loss": 0.8533,
      "step": 100500
    },
    {
      "epoch": 2.45,
      "grad_norm": 1.8805354833602905,
      "learning_rate": 1.527947142128404e-06,
      "loss": 0.8368,
      "step": 101000
    },
    {
      "epoch": 2.47,
      "grad_norm": 1.9968760013580322,
      "learning_rate": 1.520659751742901e-06,
      "loss": 0.8319,
      "step": 101500
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.8775403499603271,
      "learning_rate": 1.513372361357398e-06,
      "loss": 0.8403,
      "step": 102000
    },
    {
      "epoch": 2.49,
      "grad_norm": 1.639466404914856,
      "learning_rate": 1.5060849709718951e-06,
      "loss": 0.8451,
      "step": 102500
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.1051126718521118,
      "learning_rate": 1.498797580586392e-06,
      "loss": 0.8507,
      "step": 103000
    },
    {
      "epoch": 2.51,
      "grad_norm": 2.3899049758911133,
      "learning_rate": 1.491510190200889e-06,
      "loss": 0.8513,
      "step": 103500
    },
    {
      "epoch": 2.53,
      "grad_norm": 1.7062218189239502,
      "learning_rate": 1.484222799815386e-06,
      "loss": 0.8502,
      "step": 104000
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.6167254447937012,
      "learning_rate": 1.4769354094298832e-06,
      "loss": 0.8507,
      "step": 104500
    },
    {
      "epoch": 2.55,
      "grad_norm": 1.6577192544937134,
      "learning_rate": 1.4696480190443802e-06,
      "loss": 0.862,
      "step": 105000
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.2593485116958618,
      "learning_rate": 1.4623606286588774e-06,
      "loss": 0.8501,
      "step": 105500
    },
    {
      "epoch": 2.57,
      "grad_norm": 1.7041207551956177,
      "learning_rate": 1.4550732382733743e-06,
      "loss": 0.8401,
      "step": 106000
    },
    {
      "epoch": 2.59,
      "grad_norm": 1.9753851890563965,
      "learning_rate": 1.4477858478878715e-06,
      "loss": 0.8542,
      "step": 106500
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.3308212757110596,
      "learning_rate": 1.4404984575023685e-06,
      "loss": 0.8467,
      "step": 107000
    },
    {
      "epoch": 2.61,
      "grad_norm": 1.9371079206466675,
      "learning_rate": 1.4332110671168655e-06,
      "loss": 0.8479,
      "step": 107500
    },
    {
      "epoch": 2.62,
      "grad_norm": 2.024290084838867,
      "learning_rate": 1.4259236767313624e-06,
      "loss": 0.8512,
      "step": 108000
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.0501604080200195,
      "learning_rate": 1.4186362863458596e-06,
      "loss": 0.8445,
      "step": 108500
    },
    {
      "epoch": 2.65,
      "grad_norm": 2.04934024810791,
      "learning_rate": 1.4113488959603566e-06,
      "loss": 0.838,
      "step": 109000
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.269510269165039,
      "learning_rate": 1.4040615055748536e-06,
      "loss": 0.8321,
      "step": 109500
    },
    {
      "epoch": 2.67,
      "grad_norm": 2.102313756942749,
      "learning_rate": 1.3967741151893508e-06,
      "loss": 0.852,
      "step": 110000
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.7929764986038208,
      "learning_rate": 1.3894867248038477e-06,
      "loss": 0.846,
      "step": 110500
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.017982006072998,
      "learning_rate": 1.382199334418345e-06,
      "loss": 0.8423,
      "step": 111000
    },
    {
      "epoch": 2.71,
      "grad_norm": 1.9013959169387817,
      "learning_rate": 1.3749119440328419e-06,
      "loss": 0.8573,
      "step": 111500
    },
    {
      "epoch": 2.72,
      "grad_norm": 1.8736122846603394,
      "learning_rate": 1.367624553647339e-06,
      "loss": 0.8349,
      "step": 112000
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.9913372993469238,
      "learning_rate": 1.360337163261836e-06,
      "loss": 0.8428,
      "step": 112500
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.4525718688964844,
      "learning_rate": 1.353049772876333e-06,
      "loss": 0.8417,
      "step": 113000
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.815102458000183,
      "learning_rate": 1.34576238249083e-06,
      "loss": 0.8389,
      "step": 113500
    },
    {
      "epoch": 2.77,
      "grad_norm": 1.5676627159118652,
      "learning_rate": 1.3384749921053272e-06,
      "loss": 0.8383,
      "step": 114000
    },
    {
      "epoch": 2.78,
      "grad_norm": 2.0817971229553223,
      "learning_rate": 1.3311876017198241e-06,
      "loss": 0.842,
      "step": 114500
    },
    {
      "epoch": 2.79,
      "grad_norm": 1.955349326133728,
      "learning_rate": 1.3239002113343211e-06,
      "loss": 0.8519,
      "step": 115000
    },
    {
      "epoch": 2.81,
      "grad_norm": 1.7214323282241821,
      "learning_rate": 1.316612820948818e-06,
      "loss": 0.8258,
      "step": 115500
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.7211090326309204,
      "learning_rate": 1.3093254305633155e-06,
      "loss": 0.8538,
      "step": 116000
    },
    {
      "epoch": 2.83,
      "grad_norm": 1.5015695095062256,
      "learning_rate": 1.3020380401778125e-06,
      "loss": 0.8474,
      "step": 116500
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.0578606128692627,
      "learning_rate": 1.2947506497923094e-06,
      "loss": 0.8349,
      "step": 117000
    },
    {
      "epoch": 2.85,
      "grad_norm": 1.6207592487335205,
      "learning_rate": 1.2874632594068064e-06,
      "loss": 0.8461,
      "step": 117500
    },
    {
      "epoch": 2.87,
      "grad_norm": 1.3865083456039429,
      "learning_rate": 1.2801758690213036e-06,
      "loss": 0.8456,
      "step": 118000
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.29143226146698,
      "learning_rate": 1.2728884786358006e-06,
      "loss": 0.8495,
      "step": 118500
    },
    {
      "epoch": 2.89,
      "grad_norm": 2.417731523513794,
      "learning_rate": 1.2656010882502975e-06,
      "loss": 0.8397,
      "step": 119000
    },
    {
      "epoch": 2.9,
      "grad_norm": 2.070009231567383,
      "learning_rate": 1.2583136978647947e-06,
      "loss": 0.8367,
      "step": 119500
    },
    {
      "epoch": 2.91,
      "grad_norm": 2.087385892868042,
      "learning_rate": 1.2510263074792917e-06,
      "loss": 0.8359,
      "step": 120000
    },
    {
      "epoch": 2.93,
      "grad_norm": 1.2374746799468994,
      "learning_rate": 1.2437389170937887e-06,
      "loss": 0.8397,
      "step": 120500
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.6379140615463257,
      "learning_rate": 1.2364515267082858e-06,
      "loss": 0.8382,
      "step": 121000
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.8691675662994385,
      "learning_rate": 1.2291641363227828e-06,
      "loss": 0.8327,
      "step": 121500
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.6968072652816772,
      "learning_rate": 1.22187674593728e-06,
      "loss": 0.8451,
      "step": 122000
    },
    {
      "epoch": 2.98,
      "grad_norm": 1.4990453720092773,
      "learning_rate": 1.214589355551777e-06,
      "loss": 0.8547,
      "step": 122500
    },
    {
      "epoch": 2.99,
      "grad_norm": 1.47952139377594,
      "learning_rate": 1.207301965166274e-06,
      "loss": 0.8588,
      "step": 123000
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.2813023328781128,
      "learning_rate": 1.2000145747807711e-06,
      "loss": 0.8424,
      "step": 123500
    },
    {
      "epoch": 3.01,
      "grad_norm": 1.3617572784423828,
      "learning_rate": 1.192727184395268e-06,
      "loss": 0.8463,
      "step": 124000
    },
    {
      "epoch": 3.02,
      "grad_norm": 1.9063286781311035,
      "learning_rate": 1.185439794009765e-06,
      "loss": 0.8482,
      "step": 124500
    },
    {
      "epoch": 3.04,
      "grad_norm": 1.670264720916748,
      "learning_rate": 1.178152403624262e-06,
      "loss": 0.8477,
      "step": 125000
    },
    {
      "epoch": 3.05,
      "grad_norm": 1.7359375953674316,
      "learning_rate": 1.1708650132387592e-06,
      "loss": 0.8582,
      "step": 125500
    },
    {
      "epoch": 3.06,
      "grad_norm": 1.4014421701431274,
      "learning_rate": 1.1635776228532562e-06,
      "loss": 0.8481,
      "step": 126000
    },
    {
      "epoch": 3.07,
      "grad_norm": 1.4907056093215942,
      "learning_rate": 1.1562902324677534e-06,
      "loss": 0.8481,
      "step": 126500
    },
    {
      "epoch": 3.08,
      "grad_norm": 1.2815477848052979,
      "learning_rate": 1.1490028420822504e-06,
      "loss": 0.8477,
      "step": 127000
    },
    {
      "epoch": 3.1,
      "grad_norm": 1.6325857639312744,
      "learning_rate": 1.1417154516967475e-06,
      "loss": 0.8357,
      "step": 127500
    },
    {
      "epoch": 3.11,
      "grad_norm": 1.9135366678237915,
      "learning_rate": 1.1344280613112445e-06,
      "loss": 0.8488,
      "step": 128000
    },
    {
      "epoch": 3.12,
      "grad_norm": 1.3671820163726807,
      "learning_rate": 1.1271406709257415e-06,
      "loss": 0.8448,
      "step": 128500
    },
    {
      "epoch": 3.13,
      "grad_norm": 1.9601062536239624,
      "learning_rate": 1.1198532805402385e-06,
      "loss": 0.8412,
      "step": 129000
    },
    {
      "epoch": 3.15,
      "grad_norm": 1.9536329507827759,
      "learning_rate": 1.1125658901547356e-06,
      "loss": 0.8521,
      "step": 129500
    },
    {
      "epoch": 3.16,
      "grad_norm": 2.6012513637542725,
      "learning_rate": 1.1052784997692326e-06,
      "loss": 0.8483,
      "step": 130000
    },
    {
      "epoch": 3.17,
      "grad_norm": 1.5790627002716064,
      "learning_rate": 1.0979911093837296e-06,
      "loss": 0.8409,
      "step": 130500
    },
    {
      "epoch": 3.18,
      "grad_norm": 1.4006590843200684,
      "learning_rate": 1.0907037189982268e-06,
      "loss": 0.8552,
      "step": 131000
    },
    {
      "epoch": 3.19,
      "grad_norm": 1.6756112575531006,
      "learning_rate": 1.083416328612724e-06,
      "loss": 0.8492,
      "step": 131500
    },
    {
      "epoch": 3.21,
      "grad_norm": 2.106093645095825,
      "learning_rate": 1.076128938227221e-06,
      "loss": 0.8494,
      "step": 132000
    },
    {
      "epoch": 3.22,
      "grad_norm": 1.9856789112091064,
      "learning_rate": 1.068841547841718e-06,
      "loss": 0.8453,
      "step": 132500
    },
    {
      "epoch": 3.23,
      "grad_norm": 1.6422119140625,
      "learning_rate": 1.0615541574562149e-06,
      "loss": 0.8432,
      "step": 133000
    },
    {
      "epoch": 3.24,
      "grad_norm": 1.478088617324829,
      "learning_rate": 1.054266767070712e-06,
      "loss": 0.8401,
      "step": 133500
    },
    {
      "epoch": 3.26,
      "grad_norm": 2.487639904022217,
      "learning_rate": 1.046979376685209e-06,
      "loss": 0.8571,
      "step": 134000
    },
    {
      "epoch": 3.27,
      "grad_norm": 1.7685463428497314,
      "learning_rate": 1.039691986299706e-06,
      "loss": 0.8501,
      "step": 134500
    },
    {
      "epoch": 3.28,
      "grad_norm": 1.670902132987976,
      "learning_rate": 1.0324045959142032e-06,
      "loss": 0.8436,
      "step": 135000
    },
    {
      "epoch": 3.29,
      "grad_norm": 2.0951144695281982,
      "learning_rate": 1.0251172055287002e-06,
      "loss": 0.8396,
      "step": 135500
    },
    {
      "epoch": 3.3,
      "grad_norm": 1.8973256349563599,
      "learning_rate": 1.0178298151431971e-06,
      "loss": 0.853,
      "step": 136000
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.164564847946167,
      "learning_rate": 1.0105424247576943e-06,
      "loss": 0.8257,
      "step": 136500
    },
    {
      "epoch": 3.33,
      "grad_norm": 1.8464789390563965,
      "learning_rate": 1.0032550343721915e-06,
      "loss": 0.8418,
      "step": 137000
    },
    {
      "epoch": 3.34,
      "grad_norm": 1.5709131956100464,
      "learning_rate": 9.959676439866885e-07,
      "loss": 0.8449,
      "step": 137500
    },
    {
      "epoch": 3.35,
      "grad_norm": 1.7511299848556519,
      "learning_rate": 9.886802536011854e-07,
      "loss": 0.8468,
      "step": 138000
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.537674069404602,
      "learning_rate": 9.813928632156824e-07,
      "loss": 0.8474,
      "step": 138500
    },
    {
      "epoch": 3.38,
      "grad_norm": 1.263789176940918,
      "learning_rate": 9.741054728301796e-07,
      "loss": 0.845,
      "step": 139000
    },
    {
      "epoch": 3.39,
      "grad_norm": 1.57870352268219,
      "learning_rate": 9.668180824446766e-07,
      "loss": 0.8373,
      "step": 139500
    },
    {
      "epoch": 3.4,
      "grad_norm": 1.4920238256454468,
      "learning_rate": 9.595306920591735e-07,
      "loss": 0.8447,
      "step": 140000
    },
    {
      "epoch": 3.41,
      "grad_norm": 1.7391154766082764,
      "learning_rate": 9.522433016736706e-07,
      "loss": 0.8482,
      "step": 140500
    },
    {
      "epoch": 3.43,
      "grad_norm": 1.6738516092300415,
      "learning_rate": 9.449559112881677e-07,
      "loss": 0.843,
      "step": 141000
    },
    {
      "epoch": 3.44,
      "grad_norm": 1.1996122598648071,
      "learning_rate": 9.376685209026649e-07,
      "loss": 0.8561,
      "step": 141500
    },
    {
      "epoch": 3.45,
      "grad_norm": 1.2781496047973633,
      "learning_rate": 9.303811305171619e-07,
      "loss": 0.8332,
      "step": 142000
    },
    {
      "epoch": 3.46,
      "grad_norm": 1.8869370222091675,
      "learning_rate": 9.230937401316589e-07,
      "loss": 0.8446,
      "step": 142500
    },
    {
      "epoch": 3.47,
      "grad_norm": 1.8429596424102783,
      "learning_rate": 9.158063497461559e-07,
      "loss": 0.8504,
      "step": 143000
    },
    {
      "epoch": 3.49,
      "grad_norm": 1.1639431715011597,
      "learning_rate": 9.08518959360653e-07,
      "loss": 0.8444,
      "step": 143500
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.859666347503662,
      "learning_rate": 9.012315689751501e-07,
      "loss": 0.8493,
      "step": 144000
    },
    {
      "epoch": 3.51,
      "grad_norm": 1.436747670173645,
      "learning_rate": 8.93944178589647e-07,
      "loss": 0.837,
      "step": 144500
    },
    {
      "epoch": 3.52,
      "grad_norm": 2.3184759616851807,
      "learning_rate": 8.866567882041441e-07,
      "loss": 0.8422,
      "step": 145000
    },
    {
      "epoch": 3.53,
      "grad_norm": 1.6610150337219238,
      "learning_rate": 8.793693978186411e-07,
      "loss": 0.8447,
      "step": 145500
    },
    {
      "epoch": 3.55,
      "grad_norm": 1.2636436223983765,
      "learning_rate": 8.720820074331382e-07,
      "loss": 0.834,
      "step": 146000
    },
    {
      "epoch": 3.56,
      "grad_norm": 1.889363169670105,
      "learning_rate": 8.647946170476351e-07,
      "loss": 0.8409,
      "step": 146500
    },
    {
      "epoch": 3.57,
      "grad_norm": 1.5473982095718384,
      "learning_rate": 8.575072266621324e-07,
      "loss": 0.8322,
      "step": 147000
    },
    {
      "epoch": 3.58,
      "grad_norm": 1.584923505783081,
      "learning_rate": 8.502198362766294e-07,
      "loss": 0.839,
      "step": 147500
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.885945439338684,
      "learning_rate": 8.429324458911265e-07,
      "loss": 0.8422,
      "step": 148000
    },
    {
      "epoch": 3.61,
      "grad_norm": 1.5852166414260864,
      "learning_rate": 8.356450555056235e-07,
      "loss": 0.8496,
      "step": 148500
    },
    {
      "epoch": 3.62,
      "grad_norm": 1.7256513833999634,
      "learning_rate": 8.283576651201205e-07,
      "loss": 0.8348,
      "step": 149000
    },
    {
      "epoch": 3.63,
      "grad_norm": 1.877148151397705,
      "learning_rate": 8.210702747346175e-07,
      "loss": 0.8308,
      "step": 149500
    },
    {
      "epoch": 3.64,
      "grad_norm": 2.022329330444336,
      "learning_rate": 8.137828843491146e-07,
      "loss": 0.8473,
      "step": 150000
    },
    {
      "epoch": 3.66,
      "grad_norm": 1.3717647790908813,
      "learning_rate": 8.064954939636116e-07,
      "loss": 0.8425,
      "step": 150500
    },
    {
      "epoch": 3.67,
      "grad_norm": 2.3695058822631836,
      "learning_rate": 7.992081035781086e-07,
      "loss": 0.8389,
      "step": 151000
    },
    {
      "epoch": 3.68,
      "grad_norm": 1.5604493618011475,
      "learning_rate": 7.919207131926057e-07,
      "loss": 0.8366,
      "step": 151500
    },
    {
      "epoch": 3.69,
      "grad_norm": 1.4562138319015503,
      "learning_rate": 7.846333228071029e-07,
      "loss": 0.8568,
      "step": 152000
    },
    {
      "epoch": 3.7,
      "grad_norm": 1.2771087884902954,
      "learning_rate": 7.773459324215999e-07,
      "loss": 0.8491,
      "step": 152500
    },
    {
      "epoch": 3.72,
      "grad_norm": 1.6236662864685059,
      "learning_rate": 7.70058542036097e-07,
      "loss": 0.8392,
      "step": 153000
    },
    {
      "epoch": 3.73,
      "grad_norm": 1.6394715309143066,
      "learning_rate": 7.627711516505939e-07,
      "loss": 0.8338,
      "step": 153500
    },
    {
      "epoch": 3.74,
      "grad_norm": 1.6152373552322388,
      "learning_rate": 7.55483761265091e-07,
      "loss": 0.8352,
      "step": 154000
    },
    {
      "epoch": 3.75,
      "grad_norm": 1.5655138492584229,
      "learning_rate": 7.481963708795881e-07,
      "loss": 0.8376,
      "step": 154500
    },
    {
      "epoch": 3.77,
      "grad_norm": 2.1247482299804688,
      "learning_rate": 7.40908980494085e-07,
      "loss": 0.8317,
      "step": 155000
    },
    {
      "epoch": 3.78,
      "grad_norm": 1.4592214822769165,
      "learning_rate": 7.336215901085821e-07,
      "loss": 0.8337,
      "step": 155500
    },
    {
      "epoch": 3.79,
      "grad_norm": 1.614206075668335,
      "learning_rate": 7.263341997230792e-07,
      "loss": 0.8467,
      "step": 156000
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.430614948272705,
      "learning_rate": 7.190468093375763e-07,
      "loss": 0.8413,
      "step": 156500
    },
    {
      "epoch": 3.81,
      "grad_norm": 1.9384502172470093,
      "learning_rate": 7.117594189520733e-07,
      "loss": 0.8504,
      "step": 157000
    },
    {
      "epoch": 3.83,
      "grad_norm": 2.411449909210205,
      "learning_rate": 7.044720285665703e-07,
      "loss": 0.8336,
      "step": 157500
    },
    {
      "epoch": 3.84,
      "grad_norm": 1.763612151145935,
      "learning_rate": 6.971846381810673e-07,
      "loss": 0.8555,
      "step": 158000
    },
    {
      "epoch": 3.85,
      "grad_norm": 1.1376973390579224,
      "learning_rate": 6.898972477955645e-07,
      "loss": 0.8554,
      "step": 158500
    },
    {
      "epoch": 3.86,
      "grad_norm": 1.9090903997421265,
      "learning_rate": 6.826098574100615e-07,
      "loss": 0.8514,
      "step": 159000
    },
    {
      "epoch": 3.87,
      "grad_norm": 1.6453237533569336,
      "learning_rate": 6.753224670245585e-07,
      "loss": 0.8437,
      "step": 159500
    },
    {
      "epoch": 3.89,
      "grad_norm": 1.9159873723983765,
      "learning_rate": 6.680350766390555e-07,
      "loss": 0.8485,
      "step": 160000
    },
    {
      "epoch": 3.9,
      "grad_norm": 1.8891979455947876,
      "learning_rate": 6.607476862535526e-07,
      "loss": 0.8528,
      "step": 160500
    },
    {
      "epoch": 3.91,
      "grad_norm": 1.7260160446166992,
      "learning_rate": 6.534602958680497e-07,
      "loss": 0.8457,
      "step": 161000
    },
    {
      "epoch": 3.92,
      "grad_norm": 2.0453617572784424,
      "learning_rate": 6.461729054825468e-07,
      "loss": 0.8521,
      "step": 161500
    },
    {
      "epoch": 3.94,
      "grad_norm": 1.504954218864441,
      "learning_rate": 6.388855150970437e-07,
      "loss": 0.8446,
      "step": 162000
    },
    {
      "epoch": 3.95,
      "grad_norm": 1.7398147583007812,
      "learning_rate": 6.315981247115408e-07,
      "loss": 0.8372,
      "step": 162500
    },
    {
      "epoch": 3.96,
      "grad_norm": 2.0305142402648926,
      "learning_rate": 6.243107343260378e-07,
      "loss": 0.8413,
      "step": 163000
    },
    {
      "epoch": 3.97,
      "grad_norm": 1.0117828845977783,
      "learning_rate": 6.17023343940535e-07,
      "loss": 0.8514,
      "step": 163500
    },
    {
      "epoch": 3.98,
      "grad_norm": 2.3209025859832764,
      "learning_rate": 6.097359535550319e-07,
      "loss": 0.8389,
      "step": 164000
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.7841107845306396,
      "learning_rate": 6.02448563169529e-07,
      "loss": 0.838,
      "step": 164500
    },
    {
      "epoch": 4.01,
      "grad_norm": 1.8195794820785522,
      "learning_rate": 5.951611727840261e-07,
      "loss": 0.8356,
      "step": 165000
    },
    {
      "epoch": 4.02,
      "grad_norm": 1.660399317741394,
      "learning_rate": 5.878737823985231e-07,
      "loss": 0.8488,
      "step": 165500
    },
    {
      "epoch": 4.03,
      "grad_norm": 1.940863013267517,
      "learning_rate": 5.805863920130201e-07,
      "loss": 0.8425,
      "step": 166000
    },
    {
      "epoch": 4.04,
      "grad_norm": 1.4398648738861084,
      "learning_rate": 5.732990016275172e-07,
      "loss": 0.8506,
      "step": 166500
    },
    {
      "epoch": 4.06,
      "grad_norm": 1.978405237197876,
      "learning_rate": 5.660116112420143e-07,
      "loss": 0.8468,
      "step": 167000
    },
    {
      "epoch": 4.07,
      "grad_norm": 1.4023778438568115,
      "learning_rate": 5.587242208565113e-07,
      "loss": 0.8291,
      "step": 167500
    },
    {
      "epoch": 4.08,
      "grad_norm": 1.769383430480957,
      "learning_rate": 5.514368304710083e-07,
      "loss": 0.8346,
      "step": 168000
    },
    {
      "epoch": 4.09,
      "grad_norm": 1.8586241006851196,
      "learning_rate": 5.441494400855054e-07,
      "loss": 0.8479,
      "step": 168500
    },
    {
      "epoch": 4.11,
      "grad_norm": 1.9255924224853516,
      "learning_rate": 5.368620497000025e-07,
      "loss": 0.8248,
      "step": 169000
    },
    {
      "epoch": 4.12,
      "grad_norm": 1.5504449605941772,
      "learning_rate": 5.295746593144995e-07,
      "loss": 0.8365,
      "step": 169500
    },
    {
      "epoch": 4.13,
      "grad_norm": 1.3311139345169067,
      "learning_rate": 5.222872689289966e-07,
      "loss": 0.8484,
      "step": 170000
    },
    {
      "epoch": 4.14,
      "grad_norm": 1.6464872360229492,
      "learning_rate": 5.149998785434935e-07,
      "loss": 0.8351,
      "step": 170500
    },
    {
      "epoch": 4.15,
      "grad_norm": 2.473436117172241,
      "learning_rate": 5.077124881579906e-07,
      "loss": 0.8367,
      "step": 171000
    },
    {
      "epoch": 4.17,
      "grad_norm": 1.344011902809143,
      "learning_rate": 5.004250977724877e-07,
      "loss": 0.846,
      "step": 171500
    },
    {
      "epoch": 4.18,
      "grad_norm": 2.196869134902954,
      "learning_rate": 4.931377073869848e-07,
      "loss": 0.8489,
      "step": 172000
    },
    {
      "epoch": 4.19,
      "grad_norm": 1.1826533079147339,
      "learning_rate": 4.858503170014817e-07,
      "loss": 0.8431,
      "step": 172500
    },
    {
      "epoch": 4.2,
      "grad_norm": 1.8212307691574097,
      "learning_rate": 4.785629266159788e-07,
      "loss": 0.8435,
      "step": 173000
    },
    {
      "epoch": 4.21,
      "grad_norm": 1.3505939245224,
      "learning_rate": 4.7127553623047584e-07,
      "loss": 0.8362,
      "step": 173500
    },
    {
      "epoch": 4.23,
      "grad_norm": 1.8151482343673706,
      "learning_rate": 4.6398814584497297e-07,
      "loss": 0.8429,
      "step": 174000
    },
    {
      "epoch": 4.24,
      "grad_norm": 2.262826919555664,
      "learning_rate": 4.5670075545947e-07,
      "loss": 0.8395,
      "step": 174500
    },
    {
      "epoch": 4.25,
      "grad_norm": 1.7509130239486694,
      "learning_rate": 4.49413365073967e-07,
      "loss": 0.8456,
      "step": 175000
    },
    {
      "epoch": 4.26,
      "grad_norm": 2.1385371685028076,
      "learning_rate": 4.4212597468846405e-07,
      "loss": 0.8565,
      "step": 175500
    },
    {
      "epoch": 4.28,
      "grad_norm": 1.9977986812591553,
      "learning_rate": 4.3483858430296107e-07,
      "loss": 0.8507,
      "step": 176000
    },
    {
      "epoch": 4.29,
      "grad_norm": 2.0747461318969727,
      "learning_rate": 4.275511939174582e-07,
      "loss": 0.8356,
      "step": 176500
    },
    {
      "epoch": 4.3,
      "grad_norm": 1.9624438285827637,
      "learning_rate": 4.2026380353195523e-07,
      "loss": 0.8511,
      "step": 177000
    },
    {
      "epoch": 4.31,
      "grad_norm": 1.4482070207595825,
      "learning_rate": 4.1297641314645226e-07,
      "loss": 0.8346,
      "step": 177500
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.470351219177246,
      "learning_rate": 4.056890227609493e-07,
      "loss": 0.8506,
      "step": 178000
    },
    {
      "epoch": 4.34,
      "grad_norm": 2.422400712966919,
      "learning_rate": 3.984016323754463e-07,
      "loss": 0.8479,
      "step": 178500
    },
    {
      "epoch": 4.35,
      "grad_norm": 1.3334782123565674,
      "learning_rate": 3.9111424198994344e-07,
      "loss": 0.8481,
      "step": 179000
    },
    {
      "epoch": 4.36,
      "grad_norm": 2.087444305419922,
      "learning_rate": 3.8382685160444046e-07,
      "loss": 0.855,
      "step": 179500
    },
    {
      "epoch": 4.37,
      "grad_norm": 1.547192931175232,
      "learning_rate": 3.765394612189375e-07,
      "loss": 0.8409,
      "step": 180000
    },
    {
      "epoch": 4.38,
      "grad_norm": 1.6010013818740845,
      "learning_rate": 3.6925207083343457e-07,
      "loss": 0.8417,
      "step": 180500
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.7114335298538208,
      "learning_rate": 3.619646804479316e-07,
      "loss": 0.8371,
      "step": 181000
    },
    {
      "epoch": 4.41,
      "grad_norm": 1.659712791442871,
      "learning_rate": 3.5467729006242867e-07,
      "loss": 0.8337,
      "step": 181500
    },
    {
      "epoch": 4.42,
      "grad_norm": 1.951274037361145,
      "learning_rate": 3.473898996769257e-07,
      "loss": 0.8443,
      "step": 182000
    },
    {
      "epoch": 4.43,
      "grad_norm": 2.3659493923187256,
      "learning_rate": 3.401025092914228e-07,
      "loss": 0.8321,
      "step": 182500
    },
    {
      "epoch": 4.45,
      "grad_norm": 1.922359585762024,
      "learning_rate": 3.328151189059198e-07,
      "loss": 0.8426,
      "step": 183000
    },
    {
      "epoch": 4.46,
      "grad_norm": 2.0098953247070312,
      "learning_rate": 3.255277285204169e-07,
      "loss": 0.851,
      "step": 183500
    },
    {
      "epoch": 4.47,
      "grad_norm": 1.6094880104064941,
      "learning_rate": 3.182403381349139e-07,
      "loss": 0.8458,
      "step": 184000
    },
    {
      "epoch": 4.48,
      "grad_norm": 1.3811109066009521,
      "learning_rate": 3.1095294774941093e-07,
      "loss": 0.8443,
      "step": 184500
    },
    {
      "epoch": 4.49,
      "grad_norm": 1.5782958269119263,
      "learning_rate": 3.03665557363908e-07,
      "loss": 0.833,
      "step": 185000
    },
    {
      "epoch": 4.51,
      "grad_norm": 1.5781981945037842,
      "learning_rate": 2.9637816697840503e-07,
      "loss": 0.8311,
      "step": 185500
    },
    {
      "epoch": 4.52,
      "grad_norm": 2.4149856567382812,
      "learning_rate": 2.890907765929021e-07,
      "loss": 0.8349,
      "step": 186000
    },
    {
      "epoch": 4.53,
      "grad_norm": 1.4584341049194336,
      "learning_rate": 2.8180338620739914e-07,
      "loss": 0.8371,
      "step": 186500
    },
    {
      "epoch": 4.54,
      "grad_norm": 1.4930590391159058,
      "learning_rate": 2.7451599582189616e-07,
      "loss": 0.8595,
      "step": 187000
    },
    {
      "epoch": 4.55,
      "grad_norm": 2.145432233810425,
      "learning_rate": 2.6722860543639324e-07,
      "loss": 0.837,
      "step": 187500
    },
    {
      "epoch": 4.57,
      "grad_norm": 1.7237696647644043,
      "learning_rate": 2.5994121505089027e-07,
      "loss": 0.8428,
      "step": 188000
    },
    {
      "epoch": 4.58,
      "grad_norm": 1.446142315864563,
      "learning_rate": 2.5265382466538734e-07,
      "loss": 0.8395,
      "step": 188500
    },
    {
      "epoch": 4.59,
      "grad_norm": 1.1841368675231934,
      "learning_rate": 2.4536643427988437e-07,
      "loss": 0.8458,
      "step": 189000
    },
    {
      "epoch": 4.6,
      "grad_norm": 2.066159725189209,
      "learning_rate": 2.3807904389438142e-07,
      "loss": 0.8381,
      "step": 189500
    },
    {
      "epoch": 4.62,
      "grad_norm": 1.5453609228134155,
      "learning_rate": 2.3079165350887847e-07,
      "loss": 0.8537,
      "step": 190000
    },
    {
      "epoch": 4.63,
      "grad_norm": 1.3462421894073486,
      "learning_rate": 2.2350426312337553e-07,
      "loss": 0.8432,
      "step": 190500
    },
    {
      "epoch": 4.64,
      "grad_norm": 2.122004508972168,
      "learning_rate": 2.162168727378726e-07,
      "loss": 0.845,
      "step": 191000
    },
    {
      "epoch": 4.65,
      "grad_norm": 1.4685583114624023,
      "learning_rate": 2.0892948235236963e-07,
      "loss": 0.8348,
      "step": 191500
    },
    {
      "epoch": 4.66,
      "grad_norm": 2.0382020473480225,
      "learning_rate": 2.0164209196686665e-07,
      "loss": 0.8442,
      "step": 192000
    },
    {
      "epoch": 4.68,
      "grad_norm": 1.715734839439392,
      "learning_rate": 1.9435470158136373e-07,
      "loss": 0.8502,
      "step": 192500
    },
    {
      "epoch": 4.69,
      "grad_norm": 1.2925180196762085,
      "learning_rate": 1.8706731119586076e-07,
      "loss": 0.8415,
      "step": 193000
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.2316107749938965,
      "learning_rate": 1.797799208103578e-07,
      "loss": 0.8422,
      "step": 193500
    },
    {
      "epoch": 4.71,
      "grad_norm": 1.557336688041687,
      "learning_rate": 1.7249253042485486e-07,
      "loss": 0.8392,
      "step": 194000
    },
    {
      "epoch": 4.72,
      "grad_norm": 1.5403544902801514,
      "learning_rate": 1.6520514003935191e-07,
      "loss": 0.8462,
      "step": 194500
    },
    {
      "epoch": 4.74,
      "grad_norm": 1.5235849618911743,
      "learning_rate": 1.5791774965384897e-07,
      "loss": 0.8308,
      "step": 195000
    },
    {
      "epoch": 4.75,
      "grad_norm": 1.7033171653747559,
      "learning_rate": 1.5063035926834602e-07,
      "loss": 0.8414,
      "step": 195500
    },
    {
      "epoch": 4.76,
      "grad_norm": 2.053173542022705,
      "learning_rate": 1.4334296888284307e-07,
      "loss": 0.8438,
      "step": 196000
    },
    {
      "epoch": 4.77,
      "grad_norm": 1.6127582788467407,
      "learning_rate": 1.3605557849734012e-07,
      "loss": 0.8354,
      "step": 196500
    },
    {
      "epoch": 4.79,
      "grad_norm": 1.8265435695648193,
      "learning_rate": 1.2876818811183715e-07,
      "loss": 0.8475,
      "step": 197000
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.6135830879211426,
      "learning_rate": 1.214807977263342e-07,
      "loss": 0.8422,
      "step": 197500
    },
    {
      "epoch": 4.81,
      "grad_norm": 1.8891984224319458,
      "learning_rate": 1.1419340734083125e-07,
      "loss": 0.8342,
      "step": 198000
    },
    {
      "epoch": 4.82,
      "grad_norm": 1.7807652950286865,
      "learning_rate": 1.069060169553283e-07,
      "loss": 0.8532,
      "step": 198500
    },
    {
      "epoch": 4.83,
      "grad_norm": 1.796636700630188,
      "learning_rate": 9.961862656982535e-08,
      "loss": 0.84,
      "step": 199000
    },
    {
      "epoch": 4.85,
      "grad_norm": 1.761971116065979,
      "learning_rate": 9.233123618432241e-08,
      "loss": 0.844,
      "step": 199500
    },
    {
      "epoch": 4.86,
      "grad_norm": 1.5903804302215576,
      "learning_rate": 8.504384579881945e-08,
      "loss": 0.8414,
      "step": 200000
    },
    {
      "epoch": 4.87,
      "grad_norm": 1.3811538219451904,
      "learning_rate": 7.77564554133165e-08,
      "loss": 0.8339,
      "step": 200500
    },
    {
      "epoch": 4.88,
      "grad_norm": 2.2250685691833496,
      "learning_rate": 7.046906502781354e-08,
      "loss": 0.8375,
      "step": 201000
    },
    {
      "epoch": 4.89,
      "grad_norm": 1.7897281646728516,
      "learning_rate": 6.318167464231059e-08,
      "loss": 0.8356,
      "step": 201500
    },
    {
      "epoch": 4.91,
      "grad_norm": 1.3008489608764648,
      "learning_rate": 5.589428425680763e-08,
      "loss": 0.8404,
      "step": 202000
    },
    {
      "epoch": 4.92,
      "grad_norm": 1.482496976852417,
      "learning_rate": 4.8606893871304685e-08,
      "loss": 0.8456,
      "step": 202500
    },
    {
      "epoch": 4.93,
      "grad_norm": 2.2471604347229004,
      "learning_rate": 4.131950348580174e-08,
      "loss": 0.8474,
      "step": 203000
    },
    {
      "epoch": 4.94,
      "grad_norm": 1.920180320739746,
      "learning_rate": 3.403211310029878e-08,
      "loss": 0.8399,
      "step": 203500
    },
    {
      "epoch": 4.96,
      "grad_norm": 1.3816407918930054,
      "learning_rate": 2.674472271479583e-08,
      "loss": 0.8443,
      "step": 204000
    },
    {
      "epoch": 4.97,
      "grad_norm": 2.0075085163116455,
      "learning_rate": 1.9457332329292883e-08,
      "loss": 0.8501,
      "step": 204500
    },
    {
      "epoch": 4.98,
      "grad_norm": 1.2477442026138306,
      "learning_rate": 1.216994194378993e-08,
      "loss": 0.8473,
      "step": 205000
    },
    {
      "epoch": 4.99,
      "grad_norm": 1.7975314855575562,
      "learning_rate": 4.882551558286977e-09,
      "loss": 0.8496,
      "step": 205500
    }
  ],
  "logging_steps": 500,
  "max_steps": 205835,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 2.1679313957397504e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
